#ifndef _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H

#include "exahype/util/CopyableAtomic.h"
#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class ADERDGCellDescription;
      class ADERDGCellDescriptionPacked;
   }
}

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   14/08/2019 19:19
 */
class exahype::records::ADERDGCellDescription { 
   
   public:

      // MANUALLY ADDED
      /**
       * Indicates if the last operation was completed for this cell description
       * and its data can be used for the next operations.
       *
       * At the moment, the traversal threads read and reset this flag
       * and the consumer threads set it.
       */
      util::CopyableAtomic<bool> _hasCompletedLastStep{true};
      
      bool getHasCompletedLastStep() const {
        return _hasCompletedLastStep.load();
      }

      void setHasCompletedLastStep(bool state) {
        _hasCompletedLastStep.store(state);
      }
      // MANUALLY ADDED


      typedef exahype::records::ADERDGCellDescriptionPacked Packed;
      
      enum CompressionState {
         Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
      };
      
      enum Creation {
         NotSpecified = 0, UniformRefinement = 1, AdaptiveRefinement = 2, AdaptiveCoarsening = 3, ReceivedDueToForkOrJoin = 4, ReceivedFromWorker = 5
      };
      
      enum Type {
         Leaf = 0, LeafChecked = 1, LeafInitiatesRefining = 2, LeafRefines = 3, LeafProlongates = 4, Parent = 5, ParentChecked = 6, ParentRequestsCoarseningA = 7, ParentRequestsCoarseningB = 8, ParentCoarsens = 9, Virtual = 10, Erased = 11
      };
      
      struct PersistentRecords {
         int _solverNumber;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
         #endif
         int _parentIndex;
         Type _type;
         Type _parentType;
         int _level;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _offset;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _size;
         #endif
         double _previousTimeStamp;
         double _previousTimeStepSize;
         double _timeStepSize;
         double _timeStamp;
         int _solutionIndex;
         int _solutionAveragesIndex;
         int _solutionCompressedIndex;
         void* _solution;
         void* _solutionAverages;
         void* _solutionCompressed;
         int _previousSolutionIndex;
         int _previousSolutionAveragesIndex;
         int _previousSolutionCompressedIndex;
         void* _previousSolution;
         void* _previousSolutionAverages;
         void* _previousSolutionCompressed;
         int _updateIndex;
         int _updateAveragesIndex;
         int _updateCompressedIndex;
         void* _update;
         void* _updateAverages;
         void* _updateCompressed;
         int _extrapolatedPredictorIndex;
         int _extrapolatedPredictorAveragesIndex;
         int _extrapolatedPredictorCompressedIndex;
         void* _extrapolatedPredictor;
         void* _extrapolatedPredictorAverages;
         void* _extrapolatedPredictorCompressed;
         int _extrapolatedPredictorGradientIndex;
         void* _extrapolatedPredictorGradient;
         int _fluctuationIndex;
         int _fluctuationAveragesIndex;
         int _fluctuationCompressedIndex;
         void* _fluctuation;
         void* _fluctuationAverages;
         void* _fluctuationCompressed;
         int _solutionMinIndex;
         int _solutionMaxIndex;
         void* _solutionMin;
         void* _solutionMax;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
         #endif
         int _augmentationStatus;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
         #endif
         int _communicationStatus;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
         #endif
         int _refinementStatus;
         int _previousRefinementStatus;
         bool _refinementFlag;
         CompressionState _compressionState;
         int _bytesPerDoFInPreviousSolution;
         int _bytesPerDoFInSolution;
         int _bytesPerDoFInUpdate;
         int _bytesPerDoFInExtrapolatedPredictor;
         int _bytesPerDoFInFluctuation;
         Creation _creation;
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const int& parentIndex, const Type& type, const Type& parentType, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousTimeStamp, const double& previousTimeStepSize, const double& timeStepSize, const double& timeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& extrapolatedPredictorGradientIndex, void* extrapolatedPredictorGradient, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _parentIndex = parentIndex;
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _type = type;
         }
         
         
         
         inline Type getParentType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _parentType;
         }
         
         
         
         inline void setParentType(const Type& parentType) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _parentType = parentType;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStamp = timeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateIndex;
         }
         
         
         
         inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateIndex = updateIndex;
         }
         
         
         
         inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateAveragesIndex;
         }
         
         
         
         inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateAveragesIndex = updateAveragesIndex;
         }
         
         
         
         inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateCompressedIndex;
         }
         
         
         
         inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateCompressedIndex = updateCompressedIndex;
         }
         
         
         
         inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _update;
         }
         
         
         
         inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _update = update;
         }
         
         
         
         inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateAverages;
         }
         
         
         
         inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateAverages = updateAverages;
         }
         
         
         
         inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateCompressed;
         }
         
         
         
         inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateCompressed = updateCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictor;
         }
         
         
         
         inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictor = extrapolatedPredictor;
         }
         
         
         
         inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorAverages;
         }
         
         
         
         inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
         }
         
         
         
         inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorCompressed;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorGradientIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorGradientIndex(const int& extrapolatedPredictorGradientIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorGradientIndex = extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictorGradient() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorGradient;
         }
         
         
         
         inline void setExtrapolatedPredictorGradient(void* extrapolatedPredictorGradient) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorGradient = extrapolatedPredictorGradient;
         }
         
         
         
         inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationIndex;
         }
         
         
         
         inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationIndex = fluctuationIndex;
         }
         
         
         
         inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationAveragesIndex;
         }
         
         
         
         inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationAveragesIndex = fluctuationAveragesIndex;
         }
         
         
         
         inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationCompressedIndex;
         }
         
         
         
         inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationCompressedIndex = fluctuationCompressedIndex;
         }
         
         
         
         inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuation;
         }
         
         
         
         inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuation = fluctuation;
         }
         
         
         
         inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationAverages;
         }
         
         
         
         inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationAverages = fluctuationAverages;
         }
         
         
         
         inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationCompressed;
         }
         
         
         
         inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationCompressed = fluctuationCompressed;
         }
         
         
         
         inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMinIndex;
         }
         
         
         
         inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMinIndex = solutionMinIndex;
         }
         
         
         
         inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMaxIndex;
         }
         
         
         
         inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMaxIndex = solutionMaxIndex;
         }
         
         
         
         inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMin;
         }
         
         
         
         inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMin = solutionMin;
         }
         
         
         
         inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMax;
         }
         
         
         
         inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMax = solutionMax;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseAugmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseAugmentationStatus = (facewiseAugmentationStatus);
         }
         
         
         
         inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _augmentationStatus;
         }
         
         
         
         inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _augmentationStatus = augmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseCommunicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseCommunicationStatus = (facewiseCommunicationStatus);
         }
         
         
         
         inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _communicationStatus;
         }
         
         
         
         inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _communicationStatus = communicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseRefinementStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseRefinementStatus = (facewiseRefinementStatus);
         }
         
         
         
         inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementStatus;
         }
         
         
         
         inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementStatus = refinementStatus;
         }
         
         
         
         inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousRefinementStatus;
         }
         
         
         
         inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousRefinementStatus = previousRefinementStatus;
         }
         
         
         
         inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementFlag;
         }
         
         
         
         inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementFlag = refinementFlag;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _compressionState;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _compressionState = compressionState;
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInSolution;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInSolution = bytesPerDoFInSolution;
         }
         
         
         
         inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInUpdate;
         }
         
         
         
         inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInUpdate = bytesPerDoFInUpdate;
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInExtrapolatedPredictor;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
         }
         
         
         
         inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInFluctuation;
         }
         
         
         
         inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
         }
         
         
         
         inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _creation;
         }
         
         
         
         inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _creation = creation;
         }
         
         
         
      };
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         ADERDGCellDescription();
         
         /**
          * Generated
          */
         ADERDGCellDescription(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         ADERDGCellDescription(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const int& parentIndex, const Type& type, const Type& parentType, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousTimeStamp, const double& previousTimeStepSize, const double& timeStepSize, const double& timeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& extrapolatedPredictorGradientIndex, void* extrapolatedPredictorGradient, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
         
         /**
          * Generated
          */
         ~ADERDGCellDescription();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._neighbourMergePerformed[elementIndex];
            
         }
         
         
         
         inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
            
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._type = type;
         }
         
         
         
         inline Type getParentType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentType;
         }
         
         
         
         inline void setParentType(const Type& parentType) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentType = parentType;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStamp = timeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateIndex;
         }
         
         
         
         inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateIndex = updateIndex;
         }
         
         
         
         inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateAveragesIndex;
         }
         
         
         
         inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateAveragesIndex = updateAveragesIndex;
         }
         
         
         
         inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateCompressedIndex;
         }
         
         
         
         inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateCompressedIndex = updateCompressedIndex;
         }
         
         
         
         inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._update;
         }
         
         
         
         inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._update = update;
         }
         
         
         
         inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateAverages;
         }
         
         
         
         inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateAverages = updateAverages;
         }
         
         
         
         inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateCompressed;
         }
         
         
         
         inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateCompressed = updateCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictor;
         }
         
         
         
         inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
         }
         
         
         
         inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorAverages;
         }
         
         
         
         inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
         }
         
         
         
         inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorCompressed;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorGradientIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorGradientIndex(const int& extrapolatedPredictorGradientIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorGradientIndex = extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictorGradient() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorGradient;
         }
         
         
         
         inline void setExtrapolatedPredictorGradient(void* extrapolatedPredictorGradient) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorGradient = extrapolatedPredictorGradient;
         }
         
         
         
         inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationIndex;
         }
         
         
         
         inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationIndex = fluctuationIndex;
         }
         
         
         
         inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationAveragesIndex;
         }
         
         
         
         inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
         }
         
         
         
         inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationCompressedIndex;
         }
         
         
         
         inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
         }
         
         
         
         inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuation;
         }
         
         
         
         inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuation = fluctuation;
         }
         
         
         
         inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationAverages;
         }
         
         
         
         inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationAverages = fluctuationAverages;
         }
         
         
         
         inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationCompressed;
         }
         
         
         
         inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationCompressed = fluctuationCompressed;
         }
         
         
         
         inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMinIndex;
         }
         
         
         
         inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMinIndex = solutionMinIndex;
         }
         
         
         
         inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMaxIndex;
         }
         
         
         
         inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMaxIndex = solutionMaxIndex;
         }
         
         
         
         inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMin;
         }
         
         
         
         inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMin = solutionMin;
         }
         
         
         
         inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMax;
         }
         
         
         
         inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMax = solutionMax;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseAugmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
         }
         
         
         
         inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseAugmentationStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
            
         }
         
         
         
         inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._augmentationStatus;
         }
         
         
         
         inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._augmentationStatus = augmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseCommunicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
         }
         
         
         
         inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseCommunicationStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
            
         }
         
         
         
         inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._communicationStatus;
         }
         
         
         
         inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._communicationStatus = communicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseRefinementStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
         }
         
         
         
         inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseRefinementStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
            
         }
         
         
         
         inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementStatus;
         }
         
         
         
         inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementStatus = refinementStatus;
         }
         
         
         
         inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousRefinementStatus;
         }
         
         
         
         inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousRefinementStatus = previousRefinementStatus;
         }
         
         
         
         inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementFlag;
         }
         
         
         
         inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementFlag = refinementFlag;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._compressionState;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._compressionState = compressionState;
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInSolution;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
         }
         
         
         
         inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInUpdate;
         }
         
         
         
         inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInUpdate = bytesPerDoFInUpdate;
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInExtrapolatedPredictor;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
         }
         
         
         
         inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInFluctuation;
         }
         
         
         
         inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
         }
         
         
         
         inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._creation;
         }
         
         
         
         inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._creation = creation;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const CompressionState& param);
         
         /**
          * Generated
          */
         static std::string getCompressionStateMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Creation& param);
         
         /**
          * Generated
          */
         static std::string getCreationMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         ADERDGCellDescriptionPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
   
};

#ifndef DaStGenPackedPadding
  #define DaStGenPackedPadding 1      // 32 bit version
  // #define DaStGenPackedPadding 2   // 64 bit version
#endif


#ifdef PackedRecords
   #pragma pack (push, DaStGenPackedPadding)
#endif

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   14/08/2019 19:19
 */
class exahype::records::ADERDGCellDescriptionPacked { 
   
   public:
      
      typedef exahype::records::ADERDGCellDescription::Type Type;
      
      typedef exahype::records::ADERDGCellDescription::CompressionState CompressionState;
      
      typedef exahype::records::ADERDGCellDescription::Creation Creation;
      
      struct PersistentRecords {
         int _solverNumber;
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
         int _parentIndex;
         int _level;
         tarch::la::Vector<DIMENSIONS,double> _offset;
         tarch::la::Vector<DIMENSIONS,double> _size;
         double _previousTimeStamp;
         double _previousTimeStepSize;
         double _timeStepSize;
         double _timeStamp;
         int _solutionIndex;
         int _solutionAveragesIndex;
         int _solutionCompressedIndex;
         void* _solution;
         void* _solutionAverages;
         void* _solutionCompressed;
         int _previousSolutionIndex;
         int _previousSolutionAveragesIndex;
         int _previousSolutionCompressedIndex;
         void* _previousSolution;
         void* _previousSolutionAverages;
         void* _previousSolutionCompressed;
         int _updateIndex;
         int _updateAveragesIndex;
         int _updateCompressedIndex;
         void* _update;
         void* _updateAverages;
         void* _updateCompressed;
         int _extrapolatedPredictorIndex;
         int _extrapolatedPredictorAveragesIndex;
         int _extrapolatedPredictorCompressedIndex;
         void* _extrapolatedPredictor;
         void* _extrapolatedPredictorAverages;
         void* _extrapolatedPredictorCompressed;
         int _extrapolatedPredictorGradientIndex;
         void* _extrapolatedPredictorGradient;
         int _fluctuationIndex;
         int _fluctuationAveragesIndex;
         int _fluctuationCompressedIndex;
         void* _fluctuation;
         void* _fluctuationAverages;
         void* _fluctuationCompressed;
         int _solutionMinIndex;
         int _solutionMaxIndex;
         void* _solutionMin;
         void* _solutionMax;
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
         int _augmentationStatus;
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
         int _communicationStatus;
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
         int _refinementStatus;
         int _previousRefinementStatus;
         bool _refinementFlag;
         Creation _creation;
         
         /** mapping of records:
         || Member 	|| startbit 	|| length
          |  type	| startbit 0	| #bits 4
          |  parentType	| startbit 4	| #bits 4
          |  compressionState	| startbit 8	| #bits 2
          |  bytesPerDoFInPreviousSolution	| startbit 10	| #bits 3
          |  bytesPerDoFInSolution	| startbit 13	| #bits 3
          |  bytesPerDoFInUpdate	| startbit 16	| #bits 3
          |  bytesPerDoFInExtrapolatedPredictor	| startbit 19	| #bits 3
          |  bytesPerDoFInFluctuation	| startbit 22	| #bits 3
          */
         int _packedRecords0;
         
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const int& parentIndex, const Type& type, const Type& parentType, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousTimeStamp, const double& previousTimeStepSize, const double& timeStepSize, const double& timeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& extrapolatedPredictorGradientIndex, void* extrapolatedPredictorGradient, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _parentIndex = parentIndex;
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 11));
   return (Type) tmp;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((type >= 0 && type <= 11));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (0));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(type) << (0));
         }
         
         
         
         inline Type getParentType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 11));
   return (Type) tmp;
         }
         
         
         
         inline void setParentType(const Type& parentType) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((parentType >= 0 && parentType <= 11));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(parentType) << (4));
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStamp = timeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateIndex;
         }
         
         
         
         inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateIndex = updateIndex;
         }
         
         
         
         inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateAveragesIndex;
         }
         
         
         
         inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateAveragesIndex = updateAveragesIndex;
         }
         
         
         
         inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateCompressedIndex;
         }
         
         
         
         inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateCompressedIndex = updateCompressedIndex;
         }
         
         
         
         inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _update;
         }
         
         
         
         inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _update = update;
         }
         
         
         
         inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateAverages;
         }
         
         
         
         inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateAverages = updateAverages;
         }
         
         
         
         inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateCompressed;
         }
         
         
         
         inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateCompressed = updateCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictor;
         }
         
         
         
         inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictor = extrapolatedPredictor;
         }
         
         
         
         inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorAverages;
         }
         
         
         
         inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
         }
         
         
         
         inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorCompressed;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorGradientIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorGradientIndex(const int& extrapolatedPredictorGradientIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorGradientIndex = extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictorGradient() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorGradient;
         }
         
         
         
         inline void setExtrapolatedPredictorGradient(void* extrapolatedPredictorGradient) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorGradient = extrapolatedPredictorGradient;
         }
         
         
         
         inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationIndex;
         }
         
         
         
         inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationIndex = fluctuationIndex;
         }
         
         
         
         inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationAveragesIndex;
         }
         
         
         
         inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationAveragesIndex = fluctuationAveragesIndex;
         }
         
         
         
         inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationCompressedIndex;
         }
         
         
         
         inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationCompressedIndex = fluctuationCompressedIndex;
         }
         
         
         
         inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuation;
         }
         
         
         
         inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuation = fluctuation;
         }
         
         
         
         inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationAverages;
         }
         
         
         
         inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationAverages = fluctuationAverages;
         }
         
         
         
         inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationCompressed;
         }
         
         
         
         inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationCompressed = fluctuationCompressed;
         }
         
         
         
         inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMinIndex;
         }
         
         
         
         inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMinIndex = solutionMinIndex;
         }
         
         
         
         inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMaxIndex;
         }
         
         
         
         inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMaxIndex = solutionMaxIndex;
         }
         
         
         
         inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMin;
         }
         
         
         
         inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMin = solutionMin;
         }
         
         
         
         inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionMax;
         }
         
         
         
         inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionMax = solutionMax;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseAugmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseAugmentationStatus = (facewiseAugmentationStatus);
         }
         
         
         
         inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _augmentationStatus;
         }
         
         
         
         inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _augmentationStatus = augmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseCommunicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseCommunicationStatus = (facewiseCommunicationStatus);
         }
         
         
         
         inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _communicationStatus;
         }
         
         
         
         inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _communicationStatus = communicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _facewiseRefinementStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _facewiseRefinementStatus = (facewiseRefinementStatus);
         }
         
         
         
         inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementStatus;
         }
         
         
         
         inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementStatus = refinementStatus;
         }
         
         
         
         inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousRefinementStatus;
         }
         
         
         
         inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousRefinementStatus = previousRefinementStatus;
         }
         
         
         
         inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementFlag;
         }
         
         
         
         inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementFlag = refinementFlag;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (8));
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
         }
         
         
         
         inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
         }
         
         
         
         inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
         }
         
         
         
         inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _creation;
         }
         
         
         
         inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _creation = creation;
         }
         
         
         
      };
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         ADERDGCellDescriptionPacked();
         
         /**
          * Generated
          */
         ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         ADERDGCellDescriptionPacked(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const int& parentIndex, const Type& type, const Type& parentType, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousTimeStamp, const double& previousTimeStepSize, const double& timeStepSize, const double& timeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& extrapolatedPredictorGradientIndex, void* extrapolatedPredictorGradient, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
         
         /**
          * Generated
          */
         ~ADERDGCellDescriptionPacked();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._neighbourMergePerformed[elementIndex];
            
         }
         
         
         
         inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
            
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 11));
   return (Type) tmp;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((type >= 0 && type <= 11));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (0));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(type) << (0));
         }
         
         
         
         inline Type getParentType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 11));
   return (Type) tmp;
         }
         
         
         
         inline void setParentType(const Type& parentType) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((parentType >= 0 && parentType <= 11));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(parentType) << (4));
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStamp = timeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateIndex;
         }
         
         
         
         inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateIndex = updateIndex;
         }
         
         
         
         inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateAveragesIndex;
         }
         
         
         
         inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateAveragesIndex = updateAveragesIndex;
         }
         
         
         
         inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateCompressedIndex;
         }
         
         
         
         inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateCompressedIndex = updateCompressedIndex;
         }
         
         
         
         inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._update;
         }
         
         
         
         inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._update = update;
         }
         
         
         
         inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateAverages;
         }
         
         
         
         inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateAverages = updateAverages;
         }
         
         
         
         inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateCompressed;
         }
         
         
         
         inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateCompressed = updateCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictor;
         }
         
         
         
         inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
         }
         
         
         
         inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorAverages;
         }
         
         
         
         inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
         }
         
         
         
         inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorCompressed;
         }
         
         
         
         inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
         }
         
         
         
         inline int getExtrapolatedPredictorGradientIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void setExtrapolatedPredictorGradientIndex(const int& extrapolatedPredictorGradientIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorGradientIndex = extrapolatedPredictorGradientIndex;
         }
         
         
         
         inline void* getExtrapolatedPredictorGradient() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorGradient;
         }
         
         
         
         inline void setExtrapolatedPredictorGradient(void* extrapolatedPredictorGradient) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorGradient = extrapolatedPredictorGradient;
         }
         
         
         
         inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationIndex;
         }
         
         
         
         inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationIndex = fluctuationIndex;
         }
         
         
         
         inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationAveragesIndex;
         }
         
         
         
         inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
         }
         
         
         
         inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationCompressedIndex;
         }
         
         
         
         inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
         }
         
         
         
         inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuation;
         }
         
         
         
         inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuation = fluctuation;
         }
         
         
         
         inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationAverages;
         }
         
         
         
         inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationAverages = fluctuationAverages;
         }
         
         
         
         inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationCompressed;
         }
         
         
         
         inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationCompressed = fluctuationCompressed;
         }
         
         
         
         inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMinIndex;
         }
         
         
         
         inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMinIndex = solutionMinIndex;
         }
         
         
         
         inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMaxIndex;
         }
         
         
         
         inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMaxIndex = solutionMaxIndex;
         }
         
         
         
         inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMin;
         }
         
         
         
         inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMin = solutionMin;
         }
         
         
         
         inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMax;
         }
         
         
         
         inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMax = solutionMax;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseAugmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
         }
         
         
         
         inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseAugmentationStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
            
         }
         
         
         
         inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._augmentationStatus;
         }
         
         
         
         inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._augmentationStatus = augmentationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseCommunicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
         }
         
         
         
         inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseCommunicationStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
            
         }
         
         
         
         inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._communicationStatus;
         }
         
         
         
         inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._communicationStatus = communicationStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._facewiseRefinementStatus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
         }
         
         
         
         inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._facewiseRefinementStatus[elementIndex];
            
         }
         
         
         
         inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
            
         }
         
         
         
         inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementStatus;
         }
         
         
         
         inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementStatus = refinementStatus;
         }
         
         
         
         inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousRefinementStatus;
         }
         
         
         
         inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousRefinementStatus = previousRefinementStatus;
         }
         
         
         
         inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementFlag;
         }
         
         
         
         inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementFlag = refinementFlag;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (8));
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
         }
         
         
         
         inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
         }
         
         
         
         inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
         }
         
         
         
         inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._creation;
         }
         
         
         
         inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._creation = creation;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         static std::string toString(const CompressionState& param);
         
         /**
          * Generated
          */
         static std::string getCompressionStateMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Creation& param);
         
         /**
          * Generated
          */
         static std::string getCreationMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         ADERDGCellDescription convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
   
};

#ifdef PackedRecords
#pragma pack (pop)
#endif


#endif

