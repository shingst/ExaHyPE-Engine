#ifndef _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class ADERDGCellDescription;
      class ADERDGCellDescriptionPacked;
   }
}

#if defined(Asserts)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   18/12/2018 23:45
    */
   class exahype::records::ADERDGCellDescription { 
      
      public:
         
         typedef exahype::records::ADERDGCellDescriptionPacked Packed;
         
         enum CompressionState {
            Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
         };
         
         enum Creation {
            NotSpecified = 0, UniformRefinement = 1, AdaptiveRefinement = 2, AdaptiveCoarsening = 3, ReceivedDueToForkOrJoin = 4, ReceivedFromWorker = 5
         };
         
         enum RefinementEvent {
            None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToVirtualChildrenRequested = 3, ChangeChildrenToVirtualChildren = 4, RefiningRequested = 5, Refining = 6, Prolongating = 7, ErasingVirtualChildrenRequested = 8, ErasingVirtualChildren = 9, VirtualRefiningRequested = 10, VirtualRefining = 11, ErasingRequested = 12, Erasing = 13, ChangeToVirtualCellRequested = 14, ChangeToVirtualCell = 15, ErasingVirtualCell = 16
         };
         
         enum Type {
            Erased = 0, Ancestor = 1, Cell = 2, Descendant = 3
         };
         
         struct PersistentRecords {
            int _solverNumber;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
            #endif
            bool _hasCompletedLastStep;
            int _parentIndex;
            bool _hasVirtualChildren;
            Type _type;
            RefinementEvent _refinementEvent;
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _offset;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _size;
            #endif
            double _previousCorrectorTimeStamp;
            double _previousCorrectorTimeStepSize;
            double _correctorTimeStepSize;
            double _correctorTimeStamp;
            double _predictorTimeStepSize;
            double _predictorTimeStamp;
            int _solutionIndex;
            int _solutionAveragesIndex;
            int _solutionCompressedIndex;
            void* _solution;
            void* _solutionAverages;
            void* _solutionCompressed;
            int _previousSolutionIndex;
            int _previousSolutionAveragesIndex;
            int _previousSolutionCompressedIndex;
            void* _previousSolution;
            void* _previousSolutionAverages;
            void* _previousSolutionCompressed;
            int _updateIndex;
            int _updateAveragesIndex;
            int _updateCompressedIndex;
            void* _update;
            void* _updateAverages;
            void* _updateCompressed;
            int _extrapolatedPredictorIndex;
            int _extrapolatedPredictorAveragesIndex;
            int _extrapolatedPredictorCompressedIndex;
            void* _extrapolatedPredictor;
            void* _extrapolatedPredictorAverages;
            void* _extrapolatedPredictorCompressed;
            int _fluctuationIndex;
            int _fluctuationAveragesIndex;
            int _fluctuationCompressedIndex;
            void* _fluctuation;
            void* _fluctuationAverages;
            void* _fluctuationCompressed;
            int _solutionMinIndex;
            int _solutionMaxIndex;
            void* _solutionMin;
            void* _solutionMax;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
            #endif
            int _augmentationStatus;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
            #endif
            int _communicationStatus;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
            #endif
            int _refinementStatus;
            int _previousRefinementStatus;
            bool _refinementFlag;
            bool _vetoErasingChildren;
            int _iterationsToCureTroubledCell;
            CompressionState _compressionState;
            int _bytesPerDoFInPreviousSolution;
            int _bytesPerDoFInSolution;
            int _bytesPerDoFInUpdate;
            int _bytesPerDoFInExtrapolatedPredictor;
            int _bytesPerDoFInFluctuation;
            Creation _creation;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasCompletedLastStep;
            }
            
            
            
            inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasCompletedLastStep = hasCompletedLastStep;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasVirtualChildren;
            }
            
            
            
            inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasVirtualChildren = hasVirtualChildren;
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _type = type;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementEvent = refinementEvent;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousCorrectorTimeStamp;
            }
            
            
            
            inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousCorrectorTimeStamp = previousCorrectorTimeStamp;
            }
            
            
            
            inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousCorrectorTimeStepSize;
            }
            
            
            
            inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStepSize;
            }
            
            
            
            inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStepSize = correctorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStamp;
            }
            
            
            
            inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStamp = correctorTimeStamp;
            }
            
            
            
            inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStepSize;
            }
            
            
            
            inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStepSize = predictorTimeStepSize;
            }
            
            
            
            inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStamp;
            }
            
            
            
            inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStamp = predictorTimeStamp;
            }
            
            
            
            inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionIndex;
            }
            
            
            
            inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionIndex = solutionIndex;
            }
            
            
            
            inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAveragesIndex;
            }
            
            
            
            inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAveragesIndex = solutionAveragesIndex;
            }
            
            
            
            inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressedIndex;
            }
            
            
            
            inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressedIndex = solutionCompressedIndex;
            }
            
            
            
            inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAverages = solutionAverages;
            }
            
            
            
            inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionIndex;
            }
            
            
            
            inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionIndex = previousSolutionIndex;
            }
            
            
            
            inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAveragesIndex;
            }
            
            
            
            inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
            }
            
            
            
            inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressedIndex;
            }
            
            
            
            inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
            }
            
            
            
            inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolution;
            }
            
            
            
            inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolution = previousSolution;
            }
            
            
            
            inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateIndex;
            }
            
            
            
            inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateIndex = updateIndex;
            }
            
            
            
            inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateAveragesIndex;
            }
            
            
            
            inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateAveragesIndex = updateAveragesIndex;
            }
            
            
            
            inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateCompressedIndex;
            }
            
            
            
            inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateCompressedIndex = updateCompressedIndex;
            }
            
            
            
            inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _update;
            }
            
            
            
            inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _update = update;
            }
            
            
            
            inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateAverages;
            }
            
            
            
            inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateAverages = updateAverages;
            }
            
            
            
            inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateCompressed;
            }
            
            
            
            inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateCompressed = updateCompressed;
            }
            
            
            
            inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictor;
            }
            
            
            
            inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictor = extrapolatedPredictor;
            }
            
            
            
            inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorAverages;
            }
            
            
            
            inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
            }
            
            
            
            inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorCompressed;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
            }
            
            
            
            inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationIndex;
            }
            
            
            
            inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationIndex = fluctuationIndex;
            }
            
            
            
            inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationAveragesIndex;
            }
            
            
            
            inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationAveragesIndex = fluctuationAveragesIndex;
            }
            
            
            
            inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationCompressedIndex;
            }
            
            
            
            inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationCompressedIndex = fluctuationCompressedIndex;
            }
            
            
            
            inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuation;
            }
            
            
            
            inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuation = fluctuation;
            }
            
            
            
            inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationAverages;
            }
            
            
            
            inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationAverages = fluctuationAverages;
            }
            
            
            
            inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationCompressed;
            }
            
            
            
            inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationCompressed = fluctuationCompressed;
            }
            
            
            
            inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMinIndex;
            }
            
            
            
            inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMinIndex = solutionMinIndex;
            }
            
            
            
            inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMaxIndex;
            }
            
            
            
            inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMaxIndex = solutionMaxIndex;
            }
            
            
            
            inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMin;
            }
            
            
            
            inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMin = solutionMin;
            }
            
            
            
            inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMax;
            }
            
            
            
            inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMax = solutionMax;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseAugmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseAugmentationStatus = (facewiseAugmentationStatus);
            }
            
            
            
            inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _augmentationStatus;
            }
            
            
            
            inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _augmentationStatus = augmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseCommunicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseCommunicationStatus = (facewiseCommunicationStatus);
            }
            
            
            
            inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _communicationStatus;
            }
            
            
            
            inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _communicationStatus = communicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseRefinementStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseRefinementStatus = (facewiseRefinementStatus);
            }
            
            
            
            inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementStatus;
            }
            
            
            
            inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementStatus = refinementStatus;
            }
            
            
            
            inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousRefinementStatus;
            }
            
            
            
            inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousRefinementStatus = previousRefinementStatus;
            }
            
            
            
            inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementFlag;
            }
            
            
            
            inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementFlag = refinementFlag;
            }
            
            
            
            inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _vetoErasingChildren;
            }
            
            
            
            inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _vetoErasingChildren = vetoErasingChildren;
            }
            
            
            
            inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _iterationsToCureTroubledCell;
            }
            
            
            
            inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _iterationsToCureTroubledCell = iterationsToCureTroubledCell;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _compressionState;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _compressionState = compressionState;
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInSolution;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInSolution = bytesPerDoFInSolution;
            }
            
            
            
            inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInUpdate;
            }
            
            
            
            inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInUpdate = bytesPerDoFInUpdate;
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInExtrapolatedPredictor;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
            }
            
            
            
            inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInFluctuation;
            }
            
            
            
            inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
            }
            
            
            
            inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _creation;
            }
            
            
            
            inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _creation = creation;
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            ADERDGCellDescription();
            
            /**
             * Generated
             */
            ADERDGCellDescription(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            ADERDGCellDescription(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
            
            /**
             * Generated
             */
            ~ADERDGCellDescription();
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solverNumber = solverNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._neighbourMergePerformed[elementIndex];
               
            }
            
            
            
            inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
               
            }
            
            
            
            inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._hasCompletedLastStep;
            }
            
            
            
            inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._hasCompletedLastStep = hasCompletedLastStep;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentIndex = parentIndex;
            }
            
            
            
            inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._hasVirtualChildren;
            }
            
            
            
            inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._hasVirtualChildren = hasVirtualChildren;
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._type = type;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementEvent = refinementEvent;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._offset = (offset);
            }
            
            
            
            inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._offset[elementIndex];
               
            }
            
            
            
            inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._offset[elementIndex]= offset;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._size = (size);
            }
            
            
            
            inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._size[elementIndex];
               
            }
            
            
            
            inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._size[elementIndex]= size;
               
            }
            
            
            
            inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousCorrectorTimeStamp;
            }
            
            
            
            inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousCorrectorTimeStamp = previousCorrectorTimeStamp;
            }
            
            
            
            inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousCorrectorTimeStepSize;
            }
            
            
            
            inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._correctorTimeStepSize;
            }
            
            
            
            inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._correctorTimeStamp;
            }
            
            
            
            inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._correctorTimeStamp = correctorTimeStamp;
            }
            
            
            
            inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._predictorTimeStepSize;
            }
            
            
            
            inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
            }
            
            
            
            inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._predictorTimeStamp;
            }
            
            
            
            inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._predictorTimeStamp = predictorTimeStamp;
            }
            
            
            
            inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionIndex;
            }
            
            
            
            inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionIndex = solutionIndex;
            }
            
            
            
            inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAveragesIndex;
            }
            
            
            
            inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
            }
            
            
            
            inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressedIndex;
            }
            
            
            
            inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
            }
            
            
            
            inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solution;
            }
            
            
            
            inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solution = solution;
            }
            
            
            
            inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAverages = solutionAverages;
            }
            
            
            
            inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionIndex;
            }
            
            
            
            inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionIndex = previousSolutionIndex;
            }
            
            
            
            inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAveragesIndex;
            }
            
            
            
            inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
            }
            
            
            
            inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressedIndex;
            }
            
            
            
            inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
            }
            
            
            
            inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolution;
            }
            
            
            
            inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolution = previousSolution;
            }
            
            
            
            inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateIndex;
            }
            
            
            
            inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateIndex = updateIndex;
            }
            
            
            
            inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateAveragesIndex;
            }
            
            
            
            inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateAveragesIndex = updateAveragesIndex;
            }
            
            
            
            inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateCompressedIndex;
            }
            
            
            
            inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateCompressedIndex = updateCompressedIndex;
            }
            
            
            
            inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._update;
            }
            
            
            
            inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._update = update;
            }
            
            
            
            inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateAverages;
            }
            
            
            
            inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateAverages = updateAverages;
            }
            
            
            
            inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateCompressed;
            }
            
            
            
            inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateCompressed = updateCompressed;
            }
            
            
            
            inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictor;
            }
            
            
            
            inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
            }
            
            
            
            inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorAverages;
            }
            
            
            
            inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
            }
            
            
            
            inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorCompressed;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
            }
            
            
            
            inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationIndex;
            }
            
            
            
            inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationIndex = fluctuationIndex;
            }
            
            
            
            inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationAveragesIndex;
            }
            
            
            
            inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
            }
            
            
            
            inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationCompressedIndex;
            }
            
            
            
            inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
            }
            
            
            
            inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuation;
            }
            
            
            
            inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuation = fluctuation;
            }
            
            
            
            inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationAverages;
            }
            
            
            
            inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationAverages = fluctuationAverages;
            }
            
            
            
            inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationCompressed;
            }
            
            
            
            inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationCompressed = fluctuationCompressed;
            }
            
            
            
            inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMinIndex;
            }
            
            
            
            inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMinIndex = solutionMinIndex;
            }
            
            
            
            inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMaxIndex;
            }
            
            
            
            inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMaxIndex = solutionMaxIndex;
            }
            
            
            
            inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMin;
            }
            
            
            
            inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMin = solutionMin;
            }
            
            
            
            inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMax;
            }
            
            
            
            inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMax = solutionMax;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseAugmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
            }
            
            
            
            inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseAugmentationStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
               
            }
            
            
            
            inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._augmentationStatus;
            }
            
            
            
            inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._augmentationStatus = augmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseCommunicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
            }
            
            
            
            inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseCommunicationStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
               
            }
            
            
            
            inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._communicationStatus;
            }
            
            
            
            inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._communicationStatus = communicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseRefinementStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
            }
            
            
            
            inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseRefinementStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
               
            }
            
            
            
            inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementStatus;
            }
            
            
            
            inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementStatus = refinementStatus;
            }
            
            
            
            inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousRefinementStatus;
            }
            
            
            
            inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousRefinementStatus = previousRefinementStatus;
            }
            
            
            
            inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementFlag;
            }
            
            
            
            inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementFlag = refinementFlag;
            }
            
            
            
            inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._vetoErasingChildren;
            }
            
            
            
            inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._vetoErasingChildren = vetoErasingChildren;
            }
            
            
            
            inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._iterationsToCureTroubledCell;
            }
            
            
            
            inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._iterationsToCureTroubledCell = iterationsToCureTroubledCell;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._compressionState;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._compressionState = compressionState;
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInSolution;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
            }
            
            
            
            inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInUpdate;
            }
            
            
            
            inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInUpdate = bytesPerDoFInUpdate;
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInExtrapolatedPredictor;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
            }
            
            
            
            inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInFluctuation;
            }
            
            
            
            inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
            }
            
            
            
            inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._creation;
            }
            
            
            
            inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._creation = creation;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const CompressionState& param);
            
            /**
             * Generated
             */
            static std::string getCompressionStateMapping();
            
            /**
             * Generated
             */
            static std::string toString(const Creation& param);
            
            /**
             * Generated
             */
            static std::string getCreationMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementEvent& param);
            
            /**
             * Generated
             */
            static std::string getRefinementEventMapping();
            
            /**
             * Generated
             */
            static std::string toString(const Type& param);
            
            /**
             * Generated
             */
            static std::string getTypeMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            ADERDGCellDescriptionPacked convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               #endif
      
   };
   
   #ifndef DaStGenPackedPadding
     #define DaStGenPackedPadding 1      // 32 bit version
     // #define DaStGenPackedPadding 2   // 64 bit version
   #endif
   
   
   #ifdef PackedRecords
      #pragma pack (push, DaStGenPackedPadding)
   #endif
   
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   18/12/2018 23:45
    */
   class exahype::records::ADERDGCellDescriptionPacked { 
      
      public:
         
         typedef exahype::records::ADERDGCellDescription::Type Type;
         
         typedef exahype::records::ADERDGCellDescription::RefinementEvent RefinementEvent;
         
         typedef exahype::records::ADERDGCellDescription::CompressionState CompressionState;
         
         typedef exahype::records::ADERDGCellDescription::Creation Creation;
         
         struct PersistentRecords {
            int _solverNumber;
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
            int _parentIndex;
            bool _hasVirtualChildren;
            int _level;
            tarch::la::Vector<DIMENSIONS,double> _offset;
            tarch::la::Vector<DIMENSIONS,double> _size;
            double _previousCorrectorTimeStamp;
            double _previousCorrectorTimeStepSize;
            double _correctorTimeStepSize;
            double _correctorTimeStamp;
            double _predictorTimeStepSize;
            double _predictorTimeStamp;
            int _solutionIndex;
            int _solutionAveragesIndex;
            int _solutionCompressedIndex;
            void* _solution;
            void* _solutionAverages;
            void* _solutionCompressed;
            int _previousSolutionIndex;
            int _previousSolutionAveragesIndex;
            int _previousSolutionCompressedIndex;
            void* _previousSolution;
            void* _previousSolutionAverages;
            void* _previousSolutionCompressed;
            int _updateIndex;
            int _updateAveragesIndex;
            int _updateCompressedIndex;
            void* _update;
            void* _updateAverages;
            void* _updateCompressed;
            int _extrapolatedPredictorIndex;
            int _extrapolatedPredictorAveragesIndex;
            int _extrapolatedPredictorCompressedIndex;
            void* _extrapolatedPredictor;
            void* _extrapolatedPredictorAverages;
            void* _extrapolatedPredictorCompressed;
            int _fluctuationIndex;
            int _fluctuationAveragesIndex;
            int _fluctuationCompressedIndex;
            void* _fluctuation;
            void* _fluctuationAverages;
            void* _fluctuationCompressed;
            int _solutionMinIndex;
            int _solutionMaxIndex;
            void* _solutionMin;
            void* _solutionMax;
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
            int _augmentationStatus;
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
            int _communicationStatus;
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
            int _refinementStatus;
            int _previousRefinementStatus;
            bool _refinementFlag;
            bool _vetoErasingChildren;
            int _iterationsToCureTroubledCell;
            Creation _creation;
            
            /** mapping of records:
            || Member 	|| startbit 	|| length
             |  hasCompletedLastStep	| startbit 0	| #bits 1
             |  type	| startbit 1	| #bits 2
             |  refinementEvent	| startbit 3	| #bits 5
             |  compressionState	| startbit 8	| #bits 2
             |  bytesPerDoFInPreviousSolution	| startbit 10	| #bits 3
             |  bytesPerDoFInSolution	| startbit 13	| #bits 3
             |  bytesPerDoFInUpdate	| startbit 16	| #bits 3
             |  bytesPerDoFInExtrapolatedPredictor	| startbit 19	| #bits 3
             |  bytesPerDoFInFluctuation	| startbit 22	| #bits 3
             */
            int _packedRecords0;
            
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasVirtualChildren;
            }
            
            
            
            inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasVirtualChildren = hasVirtualChildren;
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 3));
   return (Type) tmp;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((type >= 0 && type <= 3));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(type) << (1));
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   assertion(( tmp >= 0 &&  tmp <= 16));
   return (RefinementEvent) tmp;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementEvent >= 0 && refinementEvent <= 16));
   int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementEvent) << (3));
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousCorrectorTimeStamp;
            }
            
            
            
            inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousCorrectorTimeStamp = previousCorrectorTimeStamp;
            }
            
            
            
            inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousCorrectorTimeStepSize;
            }
            
            
            
            inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStepSize;
            }
            
            
            
            inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStepSize = correctorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStamp;
            }
            
            
            
            inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStamp = correctorTimeStamp;
            }
            
            
            
            inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStepSize;
            }
            
            
            
            inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStepSize = predictorTimeStepSize;
            }
            
            
            
            inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStamp;
            }
            
            
            
            inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStamp = predictorTimeStamp;
            }
            
            
            
            inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionIndex;
            }
            
            
            
            inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionIndex = solutionIndex;
            }
            
            
            
            inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAveragesIndex;
            }
            
            
            
            inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAveragesIndex = solutionAveragesIndex;
            }
            
            
            
            inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressedIndex;
            }
            
            
            
            inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressedIndex = solutionCompressedIndex;
            }
            
            
            
            inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAverages = solutionAverages;
            }
            
            
            
            inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionIndex;
            }
            
            
            
            inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionIndex = previousSolutionIndex;
            }
            
            
            
            inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAveragesIndex;
            }
            
            
            
            inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
            }
            
            
            
            inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressedIndex;
            }
            
            
            
            inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
            }
            
            
            
            inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolution;
            }
            
            
            
            inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolution = previousSolution;
            }
            
            
            
            inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateIndex;
            }
            
            
            
            inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateIndex = updateIndex;
            }
            
            
            
            inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateAveragesIndex;
            }
            
            
            
            inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateAveragesIndex = updateAveragesIndex;
            }
            
            
            
            inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateCompressedIndex;
            }
            
            
            
            inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateCompressedIndex = updateCompressedIndex;
            }
            
            
            
            inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _update;
            }
            
            
            
            inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _update = update;
            }
            
            
            
            inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateAverages;
            }
            
            
            
            inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateAverages = updateAverages;
            }
            
            
            
            inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateCompressed;
            }
            
            
            
            inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateCompressed = updateCompressed;
            }
            
            
            
            inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictor;
            }
            
            
            
            inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictor = extrapolatedPredictor;
            }
            
            
            
            inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorAverages;
            }
            
            
            
            inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
            }
            
            
            
            inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorCompressed;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
            }
            
            
            
            inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationIndex;
            }
            
            
            
            inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationIndex = fluctuationIndex;
            }
            
            
            
            inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationAveragesIndex;
            }
            
            
            
            inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationAveragesIndex = fluctuationAveragesIndex;
            }
            
            
            
            inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationCompressedIndex;
            }
            
            
            
            inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationCompressedIndex = fluctuationCompressedIndex;
            }
            
            
            
            inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuation;
            }
            
            
            
            inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuation = fluctuation;
            }
            
            
            
            inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationAverages;
            }
            
            
            
            inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationAverages = fluctuationAverages;
            }
            
            
            
            inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationCompressed;
            }
            
            
            
            inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationCompressed = fluctuationCompressed;
            }
            
            
            
            inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMinIndex;
            }
            
            
            
            inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMinIndex = solutionMinIndex;
            }
            
            
            
            inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMaxIndex;
            }
            
            
            
            inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMaxIndex = solutionMaxIndex;
            }
            
            
            
            inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMin;
            }
            
            
            
            inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMin = solutionMin;
            }
            
            
            
            inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMax;
            }
            
            
            
            inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMax = solutionMax;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseAugmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseAugmentationStatus = (facewiseAugmentationStatus);
            }
            
            
            
            inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _augmentationStatus;
            }
            
            
            
            inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _augmentationStatus = augmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseCommunicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseCommunicationStatus = (facewiseCommunicationStatus);
            }
            
            
            
            inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _communicationStatus;
            }
            
            
            
            inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _communicationStatus = communicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _facewiseRefinementStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _facewiseRefinementStatus = (facewiseRefinementStatus);
            }
            
            
            
            inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementStatus;
            }
            
            
            
            inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementStatus = refinementStatus;
            }
            
            
            
            inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousRefinementStatus;
            }
            
            
            
            inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousRefinementStatus = previousRefinementStatus;
            }
            
            
            
            inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementFlag;
            }
            
            
            
            inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementFlag = refinementFlag;
            }
            
            
            
            inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _vetoErasingChildren;
            }
            
            
            
            inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _vetoErasingChildren = vetoErasingChildren;
            }
            
            
            
            inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _iterationsToCureTroubledCell;
            }
            
            
            
            inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _iterationsToCureTroubledCell = iterationsToCureTroubledCell;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (8));
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
            }
            
            
            
            inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
            }
            
            
            
            inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
            }
            
            
            
            inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _creation;
            }
            
            
            
            inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _creation = creation;
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            ADERDGCellDescriptionPacked();
            
            /**
             * Generated
             */
            ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            ADERDGCellDescriptionPacked(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation, const Creation& creation);
            
            /**
             * Generated
             */
            ~ADERDGCellDescriptionPacked();
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solverNumber = solverNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._neighbourMergePerformed[elementIndex];
               
            }
            
            
            
            inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
               
            }
            
            
            
            inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentIndex = parentIndex;
            }
            
            
            
            inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._hasVirtualChildren;
            }
            
            
            
            inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._hasVirtualChildren = hasVirtualChildren;
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 3));
   return (Type) tmp;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((type >= 0 && type <= 3));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(type) << (1));
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   assertion(( tmp >= 0 &&  tmp <= 16));
   return (RefinementEvent) tmp;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementEvent >= 0 && refinementEvent <= 16));
   int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementEvent) << (3));
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._offset = (offset);
            }
            
            
            
            inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._offset[elementIndex];
               
            }
            
            
            
            inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._offset[elementIndex]= offset;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._size = (size);
            }
            
            
            
            inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._size[elementIndex];
               
            }
            
            
            
            inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._size[elementIndex]= size;
               
            }
            
            
            
            inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousCorrectorTimeStamp;
            }
            
            
            
            inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousCorrectorTimeStamp = previousCorrectorTimeStamp;
            }
            
            
            
            inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousCorrectorTimeStepSize;
            }
            
            
            
            inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._correctorTimeStepSize;
            }
            
            
            
            inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._correctorTimeStamp;
            }
            
            
            
            inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._correctorTimeStamp = correctorTimeStamp;
            }
            
            
            
            inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._predictorTimeStepSize;
            }
            
            
            
            inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
            }
            
            
            
            inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._predictorTimeStamp;
            }
            
            
            
            inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._predictorTimeStamp = predictorTimeStamp;
            }
            
            
            
            inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionIndex;
            }
            
            
            
            inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionIndex = solutionIndex;
            }
            
            
            
            inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAveragesIndex;
            }
            
            
            
            inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
            }
            
            
            
            inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressedIndex;
            }
            
            
            
            inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
            }
            
            
            
            inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solution;
            }
            
            
            
            inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solution = solution;
            }
            
            
            
            inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAverages = solutionAverages;
            }
            
            
            
            inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionIndex;
            }
            
            
            
            inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionIndex = previousSolutionIndex;
            }
            
            
            
            inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAveragesIndex;
            }
            
            
            
            inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
            }
            
            
            
            inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressedIndex;
            }
            
            
            
            inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
            }
            
            
            
            inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolution;
            }
            
            
            
            inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolution = previousSolution;
            }
            
            
            
            inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateIndex;
            }
            
            
            
            inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateIndex = updateIndex;
            }
            
            
            
            inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateAveragesIndex;
            }
            
            
            
            inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateAveragesIndex = updateAveragesIndex;
            }
            
            
            
            inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateCompressedIndex;
            }
            
            
            
            inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateCompressedIndex = updateCompressedIndex;
            }
            
            
            
            inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._update;
            }
            
            
            
            inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._update = update;
            }
            
            
            
            inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateAverages;
            }
            
            
            
            inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateAverages = updateAverages;
            }
            
            
            
            inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._updateCompressed;
            }
            
            
            
            inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._updateCompressed = updateCompressed;
            }
            
            
            
            inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
            }
            
            
            
            inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
            }
            
            
            
            inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictor;
            }
            
            
            
            inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
            }
            
            
            
            inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorAverages;
            }
            
            
            
            inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
            }
            
            
            
            inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedPredictorCompressed;
            }
            
            
            
            inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
            }
            
            
            
            inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationIndex;
            }
            
            
            
            inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationIndex = fluctuationIndex;
            }
            
            
            
            inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationAveragesIndex;
            }
            
            
            
            inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
            }
            
            
            
            inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationCompressedIndex;
            }
            
            
            
            inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
            }
            
            
            
            inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuation;
            }
            
            
            
            inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuation = fluctuation;
            }
            
            
            
            inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationAverages;
            }
            
            
            
            inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationAverages = fluctuationAverages;
            }
            
            
            
            inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._fluctuationCompressed;
            }
            
            
            
            inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._fluctuationCompressed = fluctuationCompressed;
            }
            
            
            
            inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMinIndex;
            }
            
            
            
            inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMinIndex = solutionMinIndex;
            }
            
            
            
            inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMaxIndex;
            }
            
            
            
            inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMaxIndex = solutionMaxIndex;
            }
            
            
            
            inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMin;
            }
            
            
            
            inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMin = solutionMin;
            }
            
            
            
            inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionMax;
            }
            
            
            
            inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionMax = solutionMax;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseAugmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
            }
            
            
            
            inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseAugmentationStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
               
            }
            
            
            
            inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._augmentationStatus;
            }
            
            
            
            inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._augmentationStatus = augmentationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseCommunicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
            }
            
            
            
            inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseCommunicationStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
               
            }
            
            
            
            inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._communicationStatus;
            }
            
            
            
            inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._communicationStatus = communicationStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._facewiseRefinementStatus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
            }
            
            
            
            inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._facewiseRefinementStatus[elementIndex];
               
            }
            
            
            
            inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
               
            }
            
            
            
            inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementStatus;
            }
            
            
            
            inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementStatus = refinementStatus;
            }
            
            
            
            inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousRefinementStatus;
            }
            
            
            
            inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousRefinementStatus = previousRefinementStatus;
            }
            
            
            
            inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementFlag;
            }
            
            
            
            inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementFlag = refinementFlag;
            }
            
            
            
            inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._vetoErasingChildren;
            }
            
            
            
            inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._vetoErasingChildren = vetoErasingChildren;
            }
            
            
            
            inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._iterationsToCureTroubledCell;
            }
            
            
            
            inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._iterationsToCureTroubledCell = iterationsToCureTroubledCell;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (8));
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
            }
            
            
            
            inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
            }
            
            
            
            inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
            }
            
            
            
            inline Creation getCreation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._creation;
            }
            
            
            
            inline void setCreation(const Creation& creation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._creation = creation;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const Type& param);
            
            /**
             * Generated
             */
            static std::string getTypeMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementEvent& param);
            
            /**
             * Generated
             */
            static std::string getRefinementEventMapping();
            
            /**
             * Generated
             */
            static std::string toString(const CompressionState& param);
            
            /**
             * Generated
             */
            static std::string getCompressionStateMapping();
            
            /**
             * Generated
             */
            static std::string toString(const Creation& param);
            
            /**
             * Generated
             */
            static std::string getCreationMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            ADERDGCellDescription convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               #endif
      
   };
   
   #ifdef PackedRecords
   #pragma pack (pop)
   #endif
   
   
   #elif !defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   18/12/2018 23:45
       */
      class exahype::records::ADERDGCellDescription { 
         
         public:
            
            typedef exahype::records::ADERDGCellDescriptionPacked Packed;
            
            enum CompressionState {
               Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
            };
            
            enum RefinementEvent {
               None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToVirtualChildrenRequested = 3, ChangeChildrenToVirtualChildren = 4, RefiningRequested = 5, Refining = 6, Prolongating = 7, ErasingVirtualChildrenRequested = 8, ErasingVirtualChildren = 9, VirtualRefiningRequested = 10, VirtualRefining = 11, ErasingRequested = 12, Erasing = 13, ChangeToVirtualCellRequested = 14, ChangeToVirtualCell = 15, ErasingVirtualCell = 16
            };
            
            enum Type {
               Erased = 0, Ancestor = 1, Cell = 2, Descendant = 3
            };
            
            struct PersistentRecords {
               int _solverNumber;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
               #endif
               bool _hasCompletedLastStep;
               int _parentIndex;
               bool _hasVirtualChildren;
               Type _type;
               RefinementEvent _refinementEvent;
               int _level;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _offset;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _size;
               #endif
               double _previousCorrectorTimeStamp;
               double _previousCorrectorTimeStepSize;
               double _correctorTimeStepSize;
               double _correctorTimeStamp;
               double _predictorTimeStepSize;
               double _predictorTimeStamp;
               int _solutionIndex;
               int _solutionAveragesIndex;
               int _solutionCompressedIndex;
               void* _solution;
               void* _solutionAverages;
               void* _solutionCompressed;
               int _previousSolutionIndex;
               int _previousSolutionAveragesIndex;
               int _previousSolutionCompressedIndex;
               void* _previousSolution;
               void* _previousSolutionAverages;
               void* _previousSolutionCompressed;
               int _updateIndex;
               int _updateAveragesIndex;
               int _updateCompressedIndex;
               void* _update;
               void* _updateAverages;
               void* _updateCompressed;
               int _extrapolatedPredictorIndex;
               int _extrapolatedPredictorAveragesIndex;
               int _extrapolatedPredictorCompressedIndex;
               void* _extrapolatedPredictor;
               void* _extrapolatedPredictorAverages;
               void* _extrapolatedPredictorCompressed;
               int _fluctuationIndex;
               int _fluctuationAveragesIndex;
               int _fluctuationCompressedIndex;
               void* _fluctuation;
               void* _fluctuationAverages;
               void* _fluctuationCompressed;
               int _solutionMinIndex;
               int _solutionMaxIndex;
               void* _solutionMin;
               void* _solutionMax;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
               #endif
               int _augmentationStatus;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
               #endif
               int _communicationStatus;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
               #endif
               int _refinementStatus;
               int _previousRefinementStatus;
               bool _refinementFlag;
               bool _vetoErasingChildren;
               int _iterationsToCureTroubledCell;
               CompressionState _compressionState;
               int _bytesPerDoFInPreviousSolution;
               int _bytesPerDoFInSolution;
               int _bytesPerDoFInUpdate;
               int _bytesPerDoFInExtrapolatedPredictor;
               int _bytesPerDoFInFluctuation;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation);
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solverNumber = solverNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _hasCompletedLastStep;
               }
               
               
               
               inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _hasCompletedLastStep = hasCompletedLastStep;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentIndex = parentIndex;
               }
               
               
               
               inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _hasVirtualChildren;
               }
               
               
               
               inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _hasVirtualChildren = hasVirtualChildren;
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _type = type;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementEvent = refinementEvent;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _offset = (offset);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _size = (size);
               }
               
               
               
               inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousCorrectorTimeStamp;
               }
               
               
               
               inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousCorrectorTimeStamp = previousCorrectorTimeStamp;
               }
               
               
               
               inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousCorrectorTimeStepSize;
               }
               
               
               
               inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _correctorTimeStepSize;
               }
               
               
               
               inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _correctorTimeStepSize = correctorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _correctorTimeStamp;
               }
               
               
               
               inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _correctorTimeStamp = correctorTimeStamp;
               }
               
               
               
               inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _predictorTimeStepSize;
               }
               
               
               
               inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _predictorTimeStepSize = predictorTimeStepSize;
               }
               
               
               
               inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _predictorTimeStamp;
               }
               
               
               
               inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _predictorTimeStamp = predictorTimeStamp;
               }
               
               
               
               inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionIndex;
               }
               
               
               
               inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionIndex = solutionIndex;
               }
               
               
               
               inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAveragesIndex;
               }
               
               
               
               inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAveragesIndex = solutionAveragesIndex;
               }
               
               
               
               inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressedIndex;
               }
               
               
               
               inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressedIndex = solutionCompressedIndex;
               }
               
               
               
               inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solution;
               }
               
               
               
               inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solution = solution;
               }
               
               
               
               inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAverages = solutionAverages;
               }
               
               
               
               inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionIndex;
               }
               
               
               
               inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionIndex = previousSolutionIndex;
               }
               
               
               
               inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAveragesIndex;
               }
               
               
               
               inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
               }
               
               
               
               inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressedIndex;
               }
               
               
               
               inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
               }
               
               
               
               inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolution;
               }
               
               
               
               inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolution = previousSolution;
               }
               
               
               
               inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateIndex;
               }
               
               
               
               inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateIndex = updateIndex;
               }
               
               
               
               inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateAveragesIndex;
               }
               
               
               
               inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateAveragesIndex = updateAveragesIndex;
               }
               
               
               
               inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateCompressedIndex;
               }
               
               
               
               inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateCompressedIndex = updateCompressedIndex;
               }
               
               
               
               inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _update;
               }
               
               
               
               inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _update = update;
               }
               
               
               
               inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateAverages;
               }
               
               
               
               inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateAverages = updateAverages;
               }
               
               
               
               inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateCompressed;
               }
               
               
               
               inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateCompressed = updateCompressed;
               }
               
               
               
               inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictor;
               }
               
               
               
               inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictor = extrapolatedPredictor;
               }
               
               
               
               inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorAverages;
               }
               
               
               
               inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
               }
               
               
               
               inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorCompressed;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
               }
               
               
               
               inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationIndex;
               }
               
               
               
               inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationIndex = fluctuationIndex;
               }
               
               
               
               inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationAveragesIndex;
               }
               
               
               
               inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationAveragesIndex = fluctuationAveragesIndex;
               }
               
               
               
               inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationCompressedIndex;
               }
               
               
               
               inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationCompressedIndex = fluctuationCompressedIndex;
               }
               
               
               
               inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuation;
               }
               
               
               
               inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuation = fluctuation;
               }
               
               
               
               inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationAverages;
               }
               
               
               
               inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationAverages = fluctuationAverages;
               }
               
               
               
               inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationCompressed;
               }
               
               
               
               inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationCompressed = fluctuationCompressed;
               }
               
               
               
               inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMinIndex;
               }
               
               
               
               inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMinIndex = solutionMinIndex;
               }
               
               
               
               inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMaxIndex;
               }
               
               
               
               inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMaxIndex = solutionMaxIndex;
               }
               
               
               
               inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMin;
               }
               
               
               
               inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMin = solutionMin;
               }
               
               
               
               inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMax;
               }
               
               
               
               inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMax = solutionMax;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseAugmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseAugmentationStatus = (facewiseAugmentationStatus);
               }
               
               
               
               inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _augmentationStatus;
               }
               
               
               
               inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _augmentationStatus = augmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseCommunicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseCommunicationStatus = (facewiseCommunicationStatus);
               }
               
               
               
               inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _communicationStatus;
               }
               
               
               
               inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _communicationStatus = communicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseRefinementStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseRefinementStatus = (facewiseRefinementStatus);
               }
               
               
               
               inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementStatus;
               }
               
               
               
               inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementStatus = refinementStatus;
               }
               
               
               
               inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousRefinementStatus;
               }
               
               
               
               inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousRefinementStatus = previousRefinementStatus;
               }
               
               
               
               inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementFlag;
               }
               
               
               
               inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementFlag = refinementFlag;
               }
               
               
               
               inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _vetoErasingChildren;
               }
               
               
               
               inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _vetoErasingChildren = vetoErasingChildren;
               }
               
               
               
               inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _iterationsToCureTroubledCell;
               }
               
               
               
               inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _iterationsToCureTroubledCell = iterationsToCureTroubledCell;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _compressionState;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _compressionState = compressionState;
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInSolution;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInSolution = bytesPerDoFInSolution;
               }
               
               
               
               inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInUpdate;
               }
               
               
               
               inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInUpdate = bytesPerDoFInUpdate;
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInExtrapolatedPredictor;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
               }
               
               
               
               inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInFluctuation;
               }
               
               
               
               inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               ADERDGCellDescription();
               
               /**
                * Generated
                */
               ADERDGCellDescription(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               ADERDGCellDescription(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation);
               
               /**
                * Generated
                */
               ~ADERDGCellDescription();
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solverNumber = solverNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._neighbourMergePerformed[elementIndex];
                  
               }
               
               
               
               inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
                  
               }
               
               
               
               inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasCompletedLastStep;
               }
               
               
               
               inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasCompletedLastStep = hasCompletedLastStep;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentIndex = parentIndex;
               }
               
               
               
               inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasVirtualChildren;
               }
               
               
               
               inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasVirtualChildren = hasVirtualChildren;
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._type = type;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementEvent = refinementEvent;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               
               inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousCorrectorTimeStamp;
               }
               
               
               
               inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousCorrectorTimeStamp = previousCorrectorTimeStamp;
               }
               
               
               
               inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousCorrectorTimeStepSize;
               }
               
               
               
               inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._correctorTimeStepSize;
               }
               
               
               
               inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._correctorTimeStamp;
               }
               
               
               
               inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._correctorTimeStamp = correctorTimeStamp;
               }
               
               
               
               inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictorTimeStepSize;
               }
               
               
               
               inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
               }
               
               
               
               inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictorTimeStamp;
               }
               
               
               
               inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictorTimeStamp = predictorTimeStamp;
               }
               
               
               
               inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionIndex;
               }
               
               
               
               inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionIndex = solutionIndex;
               }
               
               
               
               inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAveragesIndex;
               }
               
               
               
               inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
               }
               
               
               
               inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressedIndex;
               }
               
               
               
               inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
               }
               
               
               
               inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solution;
               }
               
               
               
               inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solution = solution;
               }
               
               
               
               inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAverages = solutionAverages;
               }
               
               
               
               inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionIndex;
               }
               
               
               
               inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionIndex = previousSolutionIndex;
               }
               
               
               
               inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAveragesIndex;
               }
               
               
               
               inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
               }
               
               
               
               inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressedIndex;
               }
               
               
               
               inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
               }
               
               
               
               inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolution;
               }
               
               
               
               inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolution = previousSolution;
               }
               
               
               
               inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateIndex;
               }
               
               
               
               inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateIndex = updateIndex;
               }
               
               
               
               inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateAveragesIndex;
               }
               
               
               
               inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateAveragesIndex = updateAveragesIndex;
               }
               
               
               
               inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateCompressedIndex;
               }
               
               
               
               inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateCompressedIndex = updateCompressedIndex;
               }
               
               
               
               inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._update;
               }
               
               
               
               inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._update = update;
               }
               
               
               
               inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateAverages;
               }
               
               
               
               inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateAverages = updateAverages;
               }
               
               
               
               inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateCompressed;
               }
               
               
               
               inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateCompressed = updateCompressed;
               }
               
               
               
               inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictor;
               }
               
               
               
               inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
               }
               
               
               
               inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorAverages;
               }
               
               
               
               inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
               }
               
               
               
               inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorCompressed;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
               }
               
               
               
               inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationIndex;
               }
               
               
               
               inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationIndex = fluctuationIndex;
               }
               
               
               
               inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationAveragesIndex;
               }
               
               
               
               inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
               }
               
               
               
               inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationCompressedIndex;
               }
               
               
               
               inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
               }
               
               
               
               inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuation;
               }
               
               
               
               inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuation = fluctuation;
               }
               
               
               
               inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationAverages;
               }
               
               
               
               inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationAverages = fluctuationAverages;
               }
               
               
               
               inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationCompressed;
               }
               
               
               
               inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationCompressed = fluctuationCompressed;
               }
               
               
               
               inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMinIndex;
               }
               
               
               
               inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMinIndex = solutionMinIndex;
               }
               
               
               
               inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMaxIndex;
               }
               
               
               
               inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMaxIndex = solutionMaxIndex;
               }
               
               
               
               inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMin;
               }
               
               
               
               inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMin = solutionMin;
               }
               
               
               
               inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMax;
               }
               
               
               
               inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMax = solutionMax;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseAugmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
               }
               
               
               
               inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseAugmentationStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
                  
               }
               
               
               
               inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._augmentationStatus;
               }
               
               
               
               inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._augmentationStatus = augmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseCommunicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
               }
               
               
               
               inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseCommunicationStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
                  
               }
               
               
               
               inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._communicationStatus;
               }
               
               
               
               inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._communicationStatus = communicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseRefinementStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
               }
               
               
               
               inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseRefinementStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
                  
               }
               
               
               
               inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementStatus;
               }
               
               
               
               inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementStatus = refinementStatus;
               }
               
               
               
               inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousRefinementStatus;
               }
               
               
               
               inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousRefinementStatus = previousRefinementStatus;
               }
               
               
               
               inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementFlag;
               }
               
               
               
               inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementFlag = refinementFlag;
               }
               
               
               
               inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._vetoErasingChildren;
               }
               
               
               
               inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._vetoErasingChildren = vetoErasingChildren;
               }
               
               
               
               inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._iterationsToCureTroubledCell;
               }
               
               
               
               inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._iterationsToCureTroubledCell = iterationsToCureTroubledCell;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._compressionState;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._compressionState = compressionState;
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInSolution;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
               }
               
               
               
               inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInUpdate;
               }
               
               
               
               inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInUpdate = bytesPerDoFInUpdate;
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInExtrapolatedPredictor;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInExtrapolatedPredictor = bytesPerDoFInExtrapolatedPredictor;
               }
               
               
               
               inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInFluctuation;
               }
               
               
               
               inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInFluctuation = bytesPerDoFInFluctuation;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const CompressionState& param);
               
               /**
                * Generated
                */
               static std::string getCompressionStateMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementEvent& param);
               
               /**
                * Generated
                */
               static std::string getRefinementEventMapping();
               
               /**
                * Generated
                */
               static std::string toString(const Type& param);
               
               /**
                * Generated
                */
               static std::string getTypeMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   18/12/2018 23:45
       */
      class exahype::records::ADERDGCellDescriptionPacked { 
         
         public:
            
            typedef exahype::records::ADERDGCellDescription::Type Type;
            
            typedef exahype::records::ADERDGCellDescription::RefinementEvent RefinementEvent;
            
            typedef exahype::records::ADERDGCellDescription::CompressionState CompressionState;
            
            struct PersistentRecords {
               int _solverNumber;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
               int _parentIndex;
               bool _hasVirtualChildren;
               int _level;
               tarch::la::Vector<DIMENSIONS,double> _offset;
               tarch::la::Vector<DIMENSIONS,double> _size;
               double _previousCorrectorTimeStamp;
               double _previousCorrectorTimeStepSize;
               double _correctorTimeStepSize;
               double _correctorTimeStamp;
               double _predictorTimeStepSize;
               double _predictorTimeStamp;
               int _solutionIndex;
               int _solutionAveragesIndex;
               int _solutionCompressedIndex;
               void* _solution;
               void* _solutionAverages;
               void* _solutionCompressed;
               int _previousSolutionIndex;
               int _previousSolutionAveragesIndex;
               int _previousSolutionCompressedIndex;
               void* _previousSolution;
               void* _previousSolutionAverages;
               void* _previousSolutionCompressed;
               int _updateIndex;
               int _updateAveragesIndex;
               int _updateCompressedIndex;
               void* _update;
               void* _updateAverages;
               void* _updateCompressed;
               int _extrapolatedPredictorIndex;
               int _extrapolatedPredictorAveragesIndex;
               int _extrapolatedPredictorCompressedIndex;
               void* _extrapolatedPredictor;
               void* _extrapolatedPredictorAverages;
               void* _extrapolatedPredictorCompressed;
               int _fluctuationIndex;
               int _fluctuationAveragesIndex;
               int _fluctuationCompressedIndex;
               void* _fluctuation;
               void* _fluctuationAverages;
               void* _fluctuationCompressed;
               int _solutionMinIndex;
               int _solutionMaxIndex;
               void* _solutionMin;
               void* _solutionMax;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseAugmentationStatus;
               int _augmentationStatus;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseCommunicationStatus;
               int _communicationStatus;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _facewiseRefinementStatus;
               int _refinementStatus;
               int _previousRefinementStatus;
               bool _refinementFlag;
               bool _vetoErasingChildren;
               int _iterationsToCureTroubledCell;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  hasCompletedLastStep	| startbit 0	| #bits 1
                |  type	| startbit 1	| #bits 2
                |  refinementEvent	| startbit 3	| #bits 5
                |  compressionState	| startbit 8	| #bits 2
                |  bytesPerDoFInPreviousSolution	| startbit 10	| #bits 3
                |  bytesPerDoFInSolution	| startbit 13	| #bits 3
                |  bytesPerDoFInUpdate	| startbit 16	| #bits 3
                |  bytesPerDoFInExtrapolatedPredictor	| startbit 19	| #bits 3
                |  bytesPerDoFInFluctuation	| startbit 22	| #bits 3
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation);
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solverNumber = solverNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentIndex = parentIndex;
               }
               
               
               
               inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _hasVirtualChildren;
               }
               
               
               
               inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _hasVirtualChildren = hasVirtualChildren;
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 3));
   return (Type) tmp;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((type >= 0 && type <= 3));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(type) << (1));
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   assertion(( tmp >= 0 &&  tmp <= 16));
   return (RefinementEvent) tmp;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementEvent >= 0 && refinementEvent <= 16));
   int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementEvent) << (3));
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _offset = (offset);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _size = (size);
               }
               
               
               
               inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousCorrectorTimeStamp;
               }
               
               
               
               inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousCorrectorTimeStamp = previousCorrectorTimeStamp;
               }
               
               
               
               inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousCorrectorTimeStepSize;
               }
               
               
               
               inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _correctorTimeStepSize;
               }
               
               
               
               inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _correctorTimeStepSize = correctorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _correctorTimeStamp;
               }
               
               
               
               inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _correctorTimeStamp = correctorTimeStamp;
               }
               
               
               
               inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _predictorTimeStepSize;
               }
               
               
               
               inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _predictorTimeStepSize = predictorTimeStepSize;
               }
               
               
               
               inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _predictorTimeStamp;
               }
               
               
               
               inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _predictorTimeStamp = predictorTimeStamp;
               }
               
               
               
               inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionIndex;
               }
               
               
               
               inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionIndex = solutionIndex;
               }
               
               
               
               inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAveragesIndex;
               }
               
               
               
               inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAveragesIndex = solutionAveragesIndex;
               }
               
               
               
               inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressedIndex;
               }
               
               
               
               inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressedIndex = solutionCompressedIndex;
               }
               
               
               
               inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solution;
               }
               
               
               
               inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solution = solution;
               }
               
               
               
               inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAverages = solutionAverages;
               }
               
               
               
               inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionIndex;
               }
               
               
               
               inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionIndex = previousSolutionIndex;
               }
               
               
               
               inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAveragesIndex;
               }
               
               
               
               inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
               }
               
               
               
               inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressedIndex;
               }
               
               
               
               inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
               }
               
               
               
               inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolution;
               }
               
               
               
               inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolution = previousSolution;
               }
               
               
               
               inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateIndex;
               }
               
               
               
               inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateIndex = updateIndex;
               }
               
               
               
               inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateAveragesIndex;
               }
               
               
               
               inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateAveragesIndex = updateAveragesIndex;
               }
               
               
               
               inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateCompressedIndex;
               }
               
               
               
               inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateCompressedIndex = updateCompressedIndex;
               }
               
               
               
               inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _update;
               }
               
               
               
               inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _update = update;
               }
               
               
               
               inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateAverages;
               }
               
               
               
               inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateAverages = updateAverages;
               }
               
               
               
               inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _updateCompressed;
               }
               
               
               
               inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _updateCompressed = updateCompressed;
               }
               
               
               
               inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorIndex = extrapolatedPredictorIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictor;
               }
               
               
               
               inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictor = extrapolatedPredictor;
               }
               
               
               
               inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorAverages;
               }
               
               
               
               inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
               }
               
               
               
               inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedPredictorCompressed;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
               }
               
               
               
               inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationIndex;
               }
               
               
               
               inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationIndex = fluctuationIndex;
               }
               
               
               
               inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationAveragesIndex;
               }
               
               
               
               inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationAveragesIndex = fluctuationAveragesIndex;
               }
               
               
               
               inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationCompressedIndex;
               }
               
               
               
               inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationCompressedIndex = fluctuationCompressedIndex;
               }
               
               
               
               inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuation;
               }
               
               
               
               inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuation = fluctuation;
               }
               
               
               
               inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationAverages;
               }
               
               
               
               inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationAverages = fluctuationAverages;
               }
               
               
               
               inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _fluctuationCompressed;
               }
               
               
               
               inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _fluctuationCompressed = fluctuationCompressed;
               }
               
               
               
               inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMinIndex;
               }
               
               
               
               inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMinIndex = solutionMinIndex;
               }
               
               
               
               inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMaxIndex;
               }
               
               
               
               inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMaxIndex = solutionMaxIndex;
               }
               
               
               
               inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMin;
               }
               
               
               
               inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMin = solutionMin;
               }
               
               
               
               inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionMax;
               }
               
               
               
               inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionMax = solutionMax;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseAugmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseAugmentationStatus = (facewiseAugmentationStatus);
               }
               
               
               
               inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _augmentationStatus;
               }
               
               
               
               inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _augmentationStatus = augmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseCommunicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseCommunicationStatus = (facewiseCommunicationStatus);
               }
               
               
               
               inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _communicationStatus;
               }
               
               
               
               inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _communicationStatus = communicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _facewiseRefinementStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _facewiseRefinementStatus = (facewiseRefinementStatus);
               }
               
               
               
               inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementStatus;
               }
               
               
               
               inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementStatus = refinementStatus;
               }
               
               
               
               inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousRefinementStatus;
               }
               
               
               
               inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousRefinementStatus = previousRefinementStatus;
               }
               
               
               
               inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementFlag;
               }
               
               
               
               inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementFlag = refinementFlag;
               }
               
               
               
               inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _vetoErasingChildren;
               }
               
               
               
               inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _vetoErasingChildren = vetoErasingChildren;
               }
               
               
               
               inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _iterationsToCureTroubledCell;
               }
               
               
               
               inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _iterationsToCureTroubledCell = iterationsToCureTroubledCell;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (8));
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
               }
               
               
               
               inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
               }
               
               
               
               inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked();
               
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const int& parentIndex, const bool& hasVirtualChildren, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& previousCorrectorTimeStamp, const double& previousCorrectorTimeStepSize, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& updateIndex, const int& updateAveragesIndex, const int& updateCompressedIndex, void* update, void* updateAverages, void* updateCompressed, const int& extrapolatedPredictorIndex, const int& extrapolatedPredictorAveragesIndex, const int& extrapolatedPredictorCompressedIndex, void* extrapolatedPredictor, void* extrapolatedPredictorAverages, void* extrapolatedPredictorCompressed, const int& fluctuationIndex, const int& fluctuationAveragesIndex, const int& fluctuationCompressedIndex, void* fluctuation, void* fluctuationAverages, void* fluctuationCompressed, const int& solutionMinIndex, const int& solutionMaxIndex, void* solutionMin, void* solutionMax, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus, const int& augmentationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus, const int& communicationStatus, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus, const int& refinementStatus, const int& previousRefinementStatus, const bool& refinementFlag, const bool& vetoErasingChildren, const int& iterationsToCureTroubledCell, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInUpdate, const int& bytesPerDoFInExtrapolatedPredictor, const int& bytesPerDoFInFluctuation);
               
               /**
                * Generated
                */
               ~ADERDGCellDescriptionPacked();
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solverNumber = solverNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._neighbourMergePerformed[elementIndex];
                  
               }
               
               
               
               inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
                  
               }
               
               
               
               inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentIndex = parentIndex;
               }
               
               
               
               inline bool getHasVirtualChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasVirtualChildren;
               }
               
               
               
               inline void setHasVirtualChildren(const bool& hasVirtualChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasVirtualChildren = hasVirtualChildren;
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 3));
   return (Type) tmp;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((type >= 0 && type <= 3));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(type) << (1));
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   assertion(( tmp >= 0 &&  tmp <= 16));
   return (RefinementEvent) tmp;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementEvent >= 0 && refinementEvent <= 16));
   int mask =  (1 << (5)) - 1;
   mask = static_cast<int>(mask << (3));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementEvent) << (3));
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               
               inline double getPreviousCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousCorrectorTimeStamp;
               }
               
               
               
               inline void setPreviousCorrectorTimeStamp(const double& previousCorrectorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousCorrectorTimeStamp = previousCorrectorTimeStamp;
               }
               
               
               
               inline double getPreviousCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousCorrectorTimeStepSize;
               }
               
               
               
               inline void setPreviousCorrectorTimeStepSize(const double& previousCorrectorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousCorrectorTimeStepSize = previousCorrectorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._correctorTimeStepSize;
               }
               
               
               
               inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
               }
               
               
               
               inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._correctorTimeStamp;
               }
               
               
               
               inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._correctorTimeStamp = correctorTimeStamp;
               }
               
               
               
               inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictorTimeStepSize;
               }
               
               
               
               inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
               }
               
               
               
               inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictorTimeStamp;
               }
               
               
               
               inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictorTimeStamp = predictorTimeStamp;
               }
               
               
               
               inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionIndex;
               }
               
               
               
               inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionIndex = solutionIndex;
               }
               
               
               
               inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAveragesIndex;
               }
               
               
               
               inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
               }
               
               
               
               inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressedIndex;
               }
               
               
               
               inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
               }
               
               
               
               inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solution;
               }
               
               
               
               inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solution = solution;
               }
               
               
               
               inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAverages = solutionAverages;
               }
               
               
               
               inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionIndex;
               }
               
               
               
               inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionIndex = previousSolutionIndex;
               }
               
               
               
               inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAveragesIndex;
               }
               
               
               
               inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
               }
               
               
               
               inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressedIndex;
               }
               
               
               
               inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
               }
               
               
               
               inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolution;
               }
               
               
               
               inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolution = previousSolution;
               }
               
               
               
               inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getUpdateIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateIndex;
               }
               
               
               
               inline void setUpdateIndex(const int& updateIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateIndex = updateIndex;
               }
               
               
               
               inline int getUpdateAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateAveragesIndex;
               }
               
               
               
               inline void setUpdateAveragesIndex(const int& updateAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateAveragesIndex = updateAveragesIndex;
               }
               
               
               
               inline int getUpdateCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateCompressedIndex;
               }
               
               
               
               inline void setUpdateCompressedIndex(const int& updateCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateCompressedIndex = updateCompressedIndex;
               }
               
               
               
               inline void* getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._update;
               }
               
               
               
               inline void setUpdate(void* update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._update = update;
               }
               
               
               
               inline void* getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateAverages;
               }
               
               
               
               inline void setUpdateAverages(void* updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateAverages = updateAverages;
               }
               
               
               
               inline void* getUpdateCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateCompressed;
               }
               
               
               
               inline void setUpdateCompressed(void* updateCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateCompressed = updateCompressed;
               }
               
               
               
               inline int getExtrapolatedPredictorIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorIndex(const int& extrapolatedPredictorIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorIndex = extrapolatedPredictorIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorAveragesIndex(const int& extrapolatedPredictorAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorAveragesIndex = extrapolatedPredictorAveragesIndex;
               }
               
               
               
               inline int getExtrapolatedPredictorCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressedIndex(const int& extrapolatedPredictorCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorCompressedIndex = extrapolatedPredictorCompressedIndex;
               }
               
               
               
               inline void* getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictor;
               }
               
               
               
               inline void setExtrapolatedPredictor(void* extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
               }
               
               
               
               inline void* getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorAverages;
               }
               
               
               
               inline void setExtrapolatedPredictorAverages(void* extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
               }
               
               
               
               inline void* getExtrapolatedPredictorCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorCompressed;
               }
               
               
               
               inline void setExtrapolatedPredictorCompressed(void* extrapolatedPredictorCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorCompressed = extrapolatedPredictorCompressed;
               }
               
               
               
               inline int getFluctuationIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationIndex;
               }
               
               
               
               inline void setFluctuationIndex(const int& fluctuationIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationIndex = fluctuationIndex;
               }
               
               
               
               inline int getFluctuationAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationAveragesIndex;
               }
               
               
               
               inline void setFluctuationAveragesIndex(const int& fluctuationAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationAveragesIndex = fluctuationAveragesIndex;
               }
               
               
               
               inline int getFluctuationCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationCompressedIndex;
               }
               
               
               
               inline void setFluctuationCompressedIndex(const int& fluctuationCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationCompressedIndex = fluctuationCompressedIndex;
               }
               
               
               
               inline void* getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuation;
               }
               
               
               
               inline void setFluctuation(void* fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuation = fluctuation;
               }
               
               
               
               inline void* getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationAverages;
               }
               
               
               
               inline void setFluctuationAverages(void* fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationAverages = fluctuationAverages;
               }
               
               
               
               inline void* getFluctuationCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationCompressed;
               }
               
               
               
               inline void setFluctuationCompressed(void* fluctuationCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationCompressed = fluctuationCompressed;
               }
               
               
               
               inline int getSolutionMinIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMinIndex;
               }
               
               
               
               inline void setSolutionMinIndex(const int& solutionMinIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMinIndex = solutionMinIndex;
               }
               
               
               
               inline int getSolutionMaxIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMaxIndex;
               }
               
               
               
               inline void setSolutionMaxIndex(const int& solutionMaxIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMaxIndex = solutionMaxIndex;
               }
               
               
               
               inline void* getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMin;
               }
               
               
               
               inline void setSolutionMin(void* solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMin = solutionMin;
               }
               
               
               
               inline void* getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionMax;
               }
               
               
               
               inline void setSolutionMax(void* solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionMax = solutionMax;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseAugmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseAugmentationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseAugmentationStatus = (facewiseAugmentationStatus);
               }
               
               
               
               inline int getFacewiseAugmentationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseAugmentationStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseAugmentationStatus(int elementIndex, const int& facewiseAugmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseAugmentationStatus[elementIndex]= facewiseAugmentationStatus;
                  
               }
               
               
               
               inline int getAugmentationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._augmentationStatus;
               }
               
               
               
               inline void setAugmentationStatus(const int& augmentationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._augmentationStatus = augmentationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseCommunicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseCommunicationStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseCommunicationStatus = (facewiseCommunicationStatus);
               }
               
               
               
               inline int getFacewiseCommunicationStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseCommunicationStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseCommunicationStatus(int elementIndex, const int& facewiseCommunicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseCommunicationStatus[elementIndex]= facewiseCommunicationStatus;
                  
               }
               
               
               
               inline int getCommunicationStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._communicationStatus;
               }
               
               
               
               inline void setCommunicationStatus(const int& communicationStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._communicationStatus = communicationStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFacewiseRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._facewiseRefinementStatus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFacewiseRefinementStatus(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._facewiseRefinementStatus = (facewiseRefinementStatus);
               }
               
               
               
               inline int getFacewiseRefinementStatus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._facewiseRefinementStatus[elementIndex];
                  
               }
               
               
               
               inline void setFacewiseRefinementStatus(int elementIndex, const int& facewiseRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._facewiseRefinementStatus[elementIndex]= facewiseRefinementStatus;
                  
               }
               
               
               
               inline int getRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementStatus;
               }
               
               
               
               inline void setRefinementStatus(const int& refinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementStatus = refinementStatus;
               }
               
               
               
               inline int getPreviousRefinementStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousRefinementStatus;
               }
               
               
               
               inline void setPreviousRefinementStatus(const int& previousRefinementStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousRefinementStatus = previousRefinementStatus;
               }
               
               
               
               inline bool getRefinementFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementFlag;
               }
               
               
               
               inline void setRefinementFlag(const bool& refinementFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementFlag = refinementFlag;
               }
               
               
               
               inline bool getVetoErasingChildren() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._vetoErasingChildren;
               }
               
               
               
               inline void setVetoErasingChildren(const bool& vetoErasingChildren) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._vetoErasingChildren = vetoErasingChildren;
               }
               
               
               
               inline int getIterationsToCureTroubledCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._iterationsToCureTroubledCell;
               }
               
               
               
               inline void setIterationsToCureTroubledCell(const int& iterationsToCureTroubledCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._iterationsToCureTroubledCell = iterationsToCureTroubledCell;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (8));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (8));
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (10));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (10));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (10));
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (13));
               }
               
               
               
               inline int getBytesPerDoFInUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (16));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInUpdate(const int& bytesPerDoFInUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInUpdate >= 1 && bytesPerDoFInUpdate <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (16));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInUpdate) - 1) << (16));
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (19));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedPredictor(const int& bytesPerDoFInExtrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInExtrapolatedPredictor >= 1 && bytesPerDoFInExtrapolatedPredictor <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (19));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedPredictor) - 1) << (19));
               }
               
               
               
               inline int getBytesPerDoFInFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (22));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInFluctuation(const int& bytesPerDoFInFluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInFluctuation >= 1 && bytesPerDoFInFluctuation <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (22));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInFluctuation) - 1) << (22));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const Type& param);
               
               /**
                * Generated
                */
               static std::string getTypeMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementEvent& param);
               
               /**
                * Generated
                */
               static std::string getRefinementEventMapping();
               
               /**
                * Generated
                */
               static std::string toString(const CompressionState& param);
               
               /**
                * Generated
                */
               static std::string getCompressionStateMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               ADERDGCellDescription convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   
#endif

#endif

