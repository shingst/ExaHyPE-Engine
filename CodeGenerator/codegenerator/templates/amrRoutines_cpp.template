/**
 * This file is part of the ExaHyPE project.
 * Copyright (c) 2016  http://exahype.eu
 * All rights reserved.
 *
 * The project has received funding from the European Union's Horizon 
 * 2020 research and innovation programme under grant agreement
 * No 671698. For copyrights and licensing, please consult the webpage.
 *
 * Released under the BSD 3 Open Source License.
 * For the full license text, see LICENSE.txt
 **/
{% import 'subtemplates/macros.template' as m with context %}{# get template macros #}

#include <algorithm> //copy_n

#include "{{pathToOptKernel}}/Kernels.h"
#include "{{pathToOptKernel}}/DGMatrices.h"
#include "{{pathToOptKernel}}/Quadrature.h"

{% if useLibxsmm %}
#include "{{pathToOptKernel}}/gemmsCPP.h"
{% endif %}

// local help function
inline int powOf3(int exp){
  switch(exp) {
    case 0:
      return 1;
    case 1:
      return 3;
    case 2:
      return 9;
    case 3:
      return 27;
    case 4:
      return 81;
    case 5:
      return 243;
    default:
      int result = 243;
      for (int d=0; d<exp-5; d++) {
        result *= 3;
      }
      return result;
  }
}



void {{codeNamespace}}::faceUnknownsProlongation(
    double* restrict lQhbndFine,
    double* restrict lFhbndFine,
    const double* const restrict lQhbndCoarse,
    const double* const restrict lFhbndCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subfaceIndex
) {
  const int levelDelta = fineGridLevel - coarseGridLevel;

  // tmp arrays
  {{m.allocateArray('tmpQ', nDof*nDof3D*nDataPad) | indent(2)}}{##}
  {{m.allocateArray('tmpF', nDof*nDof3D*nVarPad ) | indent(2)}}{##}
{% if nDim == 3%}
  {{m.allocateArray('tmpX', nDof*nDof3D*nDataPad) | indent(2)}}{##}
{% endif %}

  // read only input, start with the function input = coarse
  const double* inputQ = lQhbndCoarse;
  const double* inputF = lFhbndCoarse;

  // output pointer, ensures that the output of the last iteration points to the function output
  double* outputQ;
  double* outputF;
  if (levelDelta % 2 == 0) {
    outputQ = tmpQ;
    outputF = tmpF;
  } else {
    outputQ = lQhbndFine;
    outputF = lFhbndFine;
  }

  int subfaceIndexPrevious_0 = subfaceIndex[0];
  int subfaceIndexCurrent_0;
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexPrevious_1 = subfaceIndex[1];
  int subfaceIndexCurrent_1;
  int subintervalIndex_1;
{% endif %}

  // This loop decodes the elements of subfaceIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; l++) {
    const int significance = powOf3(levelDelta-l);
    subfaceIndexCurrent_0 = subfaceIndexPrevious_0 % significance;
    subintervalIndex_0    = (subfaceIndexPrevious_0 - subfaceIndexCurrent_0)/significance;
{% if nDim==3 %}
    subfaceIndexCurrent_1 = subfaceIndexPrevious_1 % significance;
    subintervalIndex_1    = (subfaceIndexPrevious_1 - subfaceIndexCurrent_1)/significance;
{% endif %}
    
    // Apply the single level prolongation operator.
    // Use the coarse level unknowns as input in the first iteration.
{% if nDim==2 %}
    // will overwrite outputs, no need to set to 0
    {{ m.matmul('face_Q_x', 'inputQ', 'fineGridProjector1d[subintervalIndex_0]', 'outputQ', '0', '0', '0') | indent(4) }}{##}
    {{ m.matmul('face_F_x', 'inputF', 'fineGridProjector1d[subintervalIndex_0]', 'outputF', '0', '0', '0') | indent(4) }}{##}
{% else %}{# nDim == 2#}
    for (int y = 0; y < {{nDof}}; y++) {
      // will overwrite tmpX, no need to set to 0
      {{ m.matmul('face_Q_x', 'inputQ', 'fineGridProjector1d[subintervalIndex_0]', 'tmpX', 'y*'~nDof*nDataPad, '0', 'y*'~nDof*nDataPad) | indent(6) }}{##}
    }
    for (int x = 0; x < {{nDof}}; x++) {
      // will overwrite outputQ, no need to set to 0
      {{ m.matmul('face_Q_y', 'tmpX', 'fineGridProjector1d[subintervalIndex_1]', 'outputQ', 'x*'~nDataPad, '0', 'x*'~nDataPad) | indent(6) }}{##}
    }
    for (int y = 0; y < {{nDof}}; y++) {
      // will overwrite tmpX, no need to set to 0
      {{ m.matmul('face_F_x', 'inputF', 'fineGridProjector1d[subintervalIndex_0]', 'tmpX', 'y*'~nDof*nVarPad, '0', 'y*'~nDof*nVarPad) | indent(6) }}{##}
    }
    for (int x = 0; x < {{nDof}}; x++) {
      // will overwrite outputF, no need to set to 0
      {{ m.matmul('face_F_y', 'tmpX', 'fineGridProjector1d[subintervalIndex_1]', 'outputF', 'x*'~nVarPad, '0', 'x*'~nVarPad) | indent(6) }}{##}
    }
{% endif %}
    
    // Prepare next iteration.
    subfaceIndexPrevious_0 = subfaceIndexCurrent_0;
{% if nDim==3 %}
    subfaceIndexPrevious_1 = subfaceIndexCurrent_1;
{% endif %}

    // Input is previous output
    inputQ = outputQ;
    inputF = outputF;
    
    // Toggle the addresses of the pointers.
    if (outputQ == tmpQ) {
      outputQ = lQhbndFine;
      outputF = lFhbndFine;
    } else {
      outputQ = tmpQ;
      outputF = tmpF;
    }
  }
  {{m.freeArray("tmpQ")}}{##}
  {{m.freeArray("tmpF")}}{##}
{% if nDim == 3%}
  {{m.freeArray("tmpX")}}{##}
{% endif %}

}

void {{codeNamespace}}::faceFluxRestriction(
    double* restrict lFhbndCoarse,
    const double* const restrict lFhbndFine,
    const int* const subfaceIndex,
    const int levelDelta
) {
  
  // tmp array, only allocated if needed (more than one level)
  {{m.allocateArray('tmpF', nDof*nDof3D*nVarPad ) | indent(2)}}{##}
{% if nDim == 3%}
  {{m.allocateArray('tmpX', nDof*nDof3D*nDataPad) | indent(2)}}{##}
{% endif %}

  // read only input, start with the function input = fine
  const double* inputF = lFhbndFine;

  // output pointer, ensures that the output of the last iteration points to the function output
  double* outputF;
  if (levelDelta % 2 == 0) {
    outputF = tmpF;
  } else {
    outputF = lFhbndCoarse;
  }

  int subfaceIndexCurrent_0 = subfaceIndex[0];
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexCurrent_1 = subfaceIndex[1];
  int subintervalIndex_1;
{% endif %}
  
  // This loop decodes the indices of subfaceIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation loop).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; l++) {
    subintervalIndex_0    = subfaceIndexCurrent_0 % 3;  
    subfaceIndexCurrent_0 = (subfaceIndexCurrent_0 - subintervalIndex_0)/3;
{% if nDim==3 %}
    subintervalIndex_1    = subfaceIndexCurrent_1 % 3;
    subfaceIndexCurrent_1 = (subfaceIndexCurrent_1 - subintervalIndex_1)/3;
{% endif %}
    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
{% if nDim==2 %}
    // will overwrite outputs, no need to set to 0
    {{ m.matmul('face_F_x', 'inputF', 'fineGridProjector1d_T_weighted[subintervalIndex_0]', 'outputF', '0', '0', '0') | indent(4) }}{##}
{% else %}{# nDim == 2#}
    for (int y = 0; y < {{nDof}}; y++) {
      // will overwrite tmpX, no need to set to 0
      {{ m.matmul('face_F_x', 'inputF', 'fineGridProjector1d_T_weighted[subintervalIndex_0]', 'tmpX', 'y*'~nDof*nVarPad, '0', 'y*'~nDof*nVarPad) | indent(6) }}{##}
    }
    for (int x = 0; x < {{nDof}}; x++) {
      // will overwrite outputF, no need to set to 0
      {{ m.matmul('face_F_y', 'tmpX', 'fineGridProjector1d_T_weighted[subintervalIndex_1]', 'outputF', 'x*'~nVarPad, '0', 'x*'~nVarPad) | indent(6) }}{##}
    }
{% endif %}

    // Prepare next iteration.
    inputF = outputF;
    // Toggle pointer pairs.
    if (outputF == tmpF) {
      outputF = lFhbndCoarse;
    } else {
      outputF = tmpF;
    }
  }
  {{m.freeArray("tmpF")}}{##}
{% if nDim == 3%}
  {{m.freeArray("tmpX")}}{##}
{% endif %}

}


void {{codeNamespace}}::volumeUnknownsProlongation(
    double* restrict luhFine,
    const double* const restrict luhCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subcellIndex
) {
  const int levelDelta = fineGridLevel - coarseGridLevel;

  {{m.allocateArray('tmpLuh', nDof3D*nDof*nDof*nDataPad ) | indent(2)}}{##}
  {{m.allocateArray('tmpX', nDof3D*nDof*nDof*nDataPad) | indent(2)}}{##}
{% if nDim == 3 %}
  {{m.allocateArray('tmpY', nDof3D*nDof*nDof*nDataPad) | indent(2)}}{##}
{% endif %}
  
  // read only input, start with the function input = fine
  const double* inputLuh = luhCoarse;

  // output pointer, ensures that the output of the last iteration points to the function output
  double* outputLuh;
  if (levelDelta % 2 == 0) {
    outputLuh = tmpLuh;
  } else {
    outputLuh = luhFine;
  }

  int subcellIndexPrevious_0 = subcellIndex[0];
  int subcellIndexCurrent_0;
  int subintervalIndex_0;
  int subcellIndexPrevious_1 = subcellIndex[1];
  int subcellIndexCurrent_1;
  int subintervalIndex_1;
{% if nDim==3 %}
  int subcellIndexPrevious_2 = subcellIndex[2];
  int subcellIndexCurrent_2;
  int subintervalIndex_2;
{% endif %}

  // This loop step by step decodes the elements of subcellIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; l++) {
    const int significance = powOf3(levelDelta-l);
    subcellIndexCurrent_0 = subcellIndexPrevious_0 % significance;
    subintervalIndex_0    = (subcellIndexPrevious_0 - subcellIndexCurrent_0)/significance;
    subcellIndexCurrent_1 = subcellIndexPrevious_1 % significance;
    subintervalIndex_1    = (subcellIndexPrevious_1 - subcellIndexCurrent_1)/significance;
{% if nDim==3 %}
    subcellIndexCurrent_2 = subcellIndexPrevious_2 % significance;
    subintervalIndex_2    = (subcellIndexPrevious_2 - subcellIndexCurrent_2)/significance;
{% endif %}

    // will overwrite tmpX, no need to set to 0
    for (int zy = 0; zy < {{nDof*nDof3D}}; zy++) {
      {{ m.matmul('volume_x', 'inputLuh', 'fineGridProjector1d[subintervalIndex_0]', 'tmpX', 'zy*'~nDof*nData, '0', 'zy*'~nDof*nDataPad) | indent(6) }}{##}
    }
    
{% if nDim == 2 %}
    for (int x = 0; x < {{nDof}}; x++) {
      {{ m.matmul('volume_y', 'tmpX', 'fineGridProjector1d[subintervalIndex_1]', 'outputLuh', 'x*'~nDataPad, '0', 'x*'~nData) | indent(6) }}{##}
    }
{% else %}
    // will overwrite tmpY, no need to set to 0
    for (int z = 0; z < {{nDof}}; z++) {
      for (int x = 0; x < {{nDof}}; x++) {
        {{ m.matmul('volume_y', 'tmpX', 'fineGridProjector1d[subintervalIndex_1]', 'tmpY', '(z*'~nDof*nDof~'+x)*'~nDataPad, '0', '(z*'~nDof*nDof~'+x)*'~nDataPad) | indent(8) }}{##}
      }
    }
    
    for (int yx = 0; yx < {{nDof*nDof}}; yx++) {
      {{ m.matmul('volume_z', 'tmpY', 'fineGridProjector1d[subintervalIndex_2]', 'outputLuh', 'yx*'~nDataPad, '0', 'yx*'~nData) | indent(6) }}{##}
    }
{% endif %}

    // Prepare next iteration.
    subcellIndexPrevious_0 = subcellIndexCurrent_0;
    subcellIndexPrevious_1 = subcellIndexCurrent_1;
{% if nDim==3 %}
    subcellIndexPrevious_2 = subcellIndexCurrent_2;
{% endif %}

    inputLuh = outputLuh;

    // Toggle pointers.
    if (outputLuh == tmpLuh) {
      outputLuh = luhFine;
    } else {
      outputLuh = tmpLuh;
    }
  }
  {{m.freeArray("tmpLuh")}}{##}
  {{m.freeArray("tmpX")}}{##}
{% if nDim == 3%}
  {{m.freeArray("tmpY")}}{##}
{% endif %}

}


void {{codeNamespace}}::volumeUnknownsRestriction(
    double* restrict luhCoarse,
    const double* const restrict luhFine,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subcellIndex
) {
  const int levelDelta = fineGridLevel - coarseGridLevel;
  
  // read only input, start with the function input = fine
  const double* inputLuh = luhFine;
  int subintervalIndex_0 = subcellIndex[0];
  int subintervalIndex_1 = subcellIndex[1];
{% if nDim==3 %}
  int subintervalIndex_2 = subcellIndex[2];
{% endif %}

  {{m.allocateArray('tmpX', nDof3D*nDof*nDof*nDataPad) | indent(6)}}{##}
{% if nDim == 3 %}
  {{m.allocateArray('tmpY', nDof3D*nDof*nDof*nDataPad) | indent(6)}}{##}
{% endif %}
{% if not tempVarsOnStack %}{# declare them here so that they can be free later, otherwise scope error du to the if #}
  double* tmpLuh;  //allocated and freed only if levelDelta > 1
  double* tmpLuh2; //allocated and freed only if levelDelta > 1
{% endif %}


  if(levelDelta > 1) {
    {{m.allocateArray('tmpLuh', nDof3D*nDof*nDof*nDataPad, pointerExists=True ) | indent(2)}}{##}
    {{m.allocateArray('tmpLuh2', nDof3D*nDof*nDof*nDataPad, pointerExists=True ) | indent(2)}}{##}
    int subcellIndexCurrent_0 = subcellIndex[0];
    int subcellIndexCurrent_1 = subcellIndex[1];
{% if nDim==3 %}
    int subcellIndexCurrent_2 = subcellIndex[2];
{% endif %}
    // output pointer, ensures that the output of the last iteration points to the function output
    double* outputLuh = tmpLuh;
    // This loop step by step decodes the elements of subcellIndex into a tertiary basis
    // starting with the highest significance 3^(levelDelta-1).
    // 
    // Per iteration, the digits corresponding to the current significances then determine
    // the subintervals for the single level prolongation.
    for (int l = 1; l < levelDelta; l++) {
      subintervalIndex_0    = subcellIndexCurrent_0 % 3;
      subcellIndexCurrent_0 = (subcellIndexCurrent_0 - subintervalIndex_0)/3;
      subintervalIndex_1    = subcellIndexCurrent_1 % 3;
      subcellIndexCurrent_1 = (subcellIndexCurrent_1 - subintervalIndex_1)/3;
{% if nDim==3 %}
      subintervalIndex_2    = subcellIndexCurrent_2 % 3;
      subcellIndexCurrent_2 = (subcellIndexCurrent_2 - subintervalIndex_2)/3;
{% endif %}

      // will overwrite tmpX, no need to set to 0
      for (int zy = 0; zy < {{nDof3D*nDof}}; zy++) {
        {{ m.matmul('volume_x', 'inputLuh', 'fineGridProjector1d_T_weighted[subintervalIndex_0]', 'tmpX', 'zy*'~nDof*nData, '0', 'zy*'~nDof*nDataPad) | indent(8) }}{##}
      }
      
{% if nDim == 2 %}
      for (int x = 0; x < {{nDof}}; x++) {
        {{ m.matmul('volume_y', 'tmpX', 'fineGridProjector1d_T_weighted[subintervalIndex_1]', 'outputLuh', 'x*'~nDataPad, '0', 'x*'~nData) | indent(8) }}{##}
      }
{% else %}
      // will overwrite tmpY, no need to set to 0
      for (int z = 0; z < {{nDof}}; z++) {
        for (int x = 0; x < {{nDof}}; x++) {
          {{ m.matmul('volume_y', 'tmpX', 'fineGridProjector1d_T_weighted[subintervalIndex_1]', 'tmpY', '(z*'~nDof*nDof~'+x)*'~nDataPad, '0', '(z*'~nDof*nDof~'+x)*'~nDataPad) | indent(10) }}{##}
        }
      }
      for (int yx = 0; yx < {{nDof*nDof}}; yx++) {
        {{ m.matmul('volume_z', 'tmpY', 'fineGridProjector1d_T_weighted[subintervalIndex_2]', 'outputLuh', 'yx*'~nDataPad, '0', 'yx*'~nData) | indent(8) }}{##}
      }
{% endif %}

      inputLuh = outputLuh;
      // Toggle pointers.
      if (outputLuh == tmpLuh) {
        outputLuh = tmpLuh2;
      } else {
        outputLuh = tmpLuh;
      }
    }
    
    subintervalIndex_0    = subcellIndexCurrent_0 % 3;
    subintervalIndex_1    = subcellIndexCurrent_1 % 3;
{% if nDim==3 %}
    subintervalIndex_2    = subcellIndexCurrent_2 % 3;
{% endif %}
    
  } // if leveldelta>1, 
  
  //now at the case 1 level to do
  
  // will overwrite tmpX, no need to set to 0
  for (int zy = 0; zy < {{nDof3D*nDof}}; zy++) {
    {{ m.matmul('volume_x', 'inputLuh', 'fineGridProjector1d_T_weighted[subintervalIndex_0]', 'tmpX', 'zy*'~nDof*nData, '0', 'zy*'~nDof*nDataPad) | indent(4) }}{##}
  }
  
{% if nDim == 2 %}
  // Add to the coarse output
  for (int x = 0; x < {{nDof}}; x++) {
    {{ m.matmul('volume_y_add', 'tmpX', 'fineGridProjector1d_T_weighted[subintervalIndex_1]', 'luhCoarse', 'x*'~nDataPad, '0', 'x*'~nData) | indent(4) }}{##}
  }
{% else %}
  // will overwrite tmpY, no need to set to 0
  for (int z = 0; z < {{nDof}}; z++) {
    for (int x = 0; x < {{nDof}}; x++) {
      {{ m.matmul('volume_y', 'tmpX', 'fineGridProjector1d_T_weighted[subintervalIndex_1]', 'tmpY', '(z*'~nDof*nDof~'+x)*'~nDataPad, '0', '(z*'~nDof*nDof~'+x)*'~nDataPad) | indent(6) }}{##}
    }
  }
  
  // Add to the coarse output
  for (int yx = 0; yx < {{nDof*nDof}}; yx++) {
    {{ m.matmul('volume_z_add', 'tmpY', 'fineGridProjector1d_T_weighted[subintervalIndex_2]', 'luhCoarse', 'yx*'~nDataPad, '0', 'yx*'~nData) | indent(4) }}{##}
  }
{% endif %}
  
{% if not tempVarsOnStack %}
  if(levelDelta > 1) {
    {{m.freeArray("tmpLuh")}}{##}
    {{m.freeArray("tmpLuh2")}}{##}
  }
  {{m.freeArray("tmpX")}}{##}
{% if nDim == 3%}
  {{m.freeArray("tmpY")}}{##}
{% endif %}
{% endif %}

}

/**
* Legacy code
*
* TODO JMG remove later
*/


/*
void {{codeNamespace}}::faceUnknownsProlongation_old(
    double* restrict lQhbndFine,
    double* restrict lFhbndFine,
    const double* restrict lQhbndCoarse,
    const double* restrict lFhbndCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subfaceIndex
) {
  const int levelDelta = fineGridLevel - coarseGridLevel;

  double lQhbndFineTemp[{{nDof*nDof3D*nDataPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndFineTemp[{{nDof*nDof3D*nVarPad }}] __attribute__((aligned(ALIGNMENT)));
{% if nDim == 3 and useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerQhbnd1 = 0;
  double * pointerFhbnd1 = 0;

  double * pointerQhbnd2 = 0;
  double * pointerFhbnd2 = 0;

  // This ensures that the pointerQhbnd1 
  // of the last iteration points to lQhbndFine.
  // The same is done for pointerFhbnd1.
  if (levelDelta % 2 == 0) {
    pointerQhbnd1 = lQhbndFineTemp;
    pointerFhbnd1 = lFhbndFineTemp;
  } else {
    pointerQhbnd1 = lQhbndFine;
    pointerFhbnd1 = lFhbndFine;
  }

  int subfaceIndexPrevious_0 = subfaceIndex[0];
  int subfaceIndexCurrent_0;
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexPrevious_1 = subfaceIndex[1];
  int subfaceIndexCurrent_1;
  int subintervalIndex_1;
{% endif %}

  // This loop decodes the elements of subfaceIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; ++l) {
    const int significance = powOf3(levelDelta-l);
    subfaceIndexCurrent_0 = subfaceIndexPrevious_0 % significance;
    subintervalIndex_0    = (subfaceIndexPrevious_0 - subfaceIndexCurrent_0)/significance;
{% if nDim==3 %}
    subfaceIndexCurrent_1 = subfaceIndexPrevious_1 % significance;
    subintervalIndex_1    = (subfaceIndexPrevious_1 - subfaceIndexCurrent_1)/significance;
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerQhbnd1, {{nDof*nDof3D*nDataPad}}, 0.0);
    std::fill_n(pointerFhbnd1, {{nDof*nDof3D*nVarPad }}, 0.0);

    // Apply the single level prolongation operator.
    // Use the coarse level unknowns as input in the first iteration.
    if (l==1) {
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&lQhbndCoarse[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&lFhbndCoarse[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&lQhbndCoarse[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
      #pragma forceinline
#endif
          {{gemm_face_F}}(&lFhbndCoarse[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}
      // singleLevelFaceUnknownsProlongation<{{nDataPad}}>(pointerQhbnd1, lQhbndCoarse, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                    lQhbndCoarse[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }    
      //singleLevelFaceUnknownsProlongation<{{nVarPad }}>(pointerFhbnd1, lFhbndCoarse, subintervalIndex);      
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                    lFhbndCoarse[(n2*{{nDof}}+n1)*{{nVarPad }}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else { // l!= 1
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&pointerQhbnd2[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&pointerFhbnd2[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&pointerQhbnd2[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
          #pragma forceinline
#endif
          {{gemm_face_F}}(&pointerFhbnd2[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else%}
      // singleLevelFaceUnknownsProlongation<{{nDataPad}}>(pointerQhbnd1, pointerQhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                    pointerQhbnd2[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }    
      //singleLevelFaceUnknownsProlongation<{{nVarPad }}>(pointerFhbnd1, pointerFhbnd2, subintervalIndex); 
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                    pointerFhbnd2[(n2*{{nDof}}+n1)*{{nVarPad }}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }     
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    subfaceIndexPrevious_0 = subfaceIndexCurrent_0;
{% if nDim==3 %}
    subfaceIndexPrevious_1 = subfaceIndexCurrent_1;
{% endif %}

    pointerQhbnd2 = pointerQhbnd1;
    pointerFhbnd2 = pointerFhbnd1;
    
    // Toggle the addresses of the pointers.
    if (pointerQhbnd1 == lQhbndFineTemp) {
      pointerQhbnd1 = lQhbndFine;
      pointerFhbnd1 = lFhbndFine;
    } else {
      pointerQhbnd1 = lQhbndFineTemp;
      pointerFhbnd1 = lFhbndFineTemp;
    }
  }

}



void {{codeNamespace}}::faceFluxRestriction_old(
    double* restrict lFhbndCoarse,
    const double* restrict lFhbndFine,
    const int* const subfaceIndex,
    const int levelDelta
) {

  double lFhbndCoarseTemp1[{{nDof*nDof3D*nVarPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndCoarseTemp2[{{nDof*nDof3D*nVarPad}}] __attribute__((aligned(ALIGNMENT)));
  
{% if nDim == 3 and useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerFhbnd1 = 0;
  double * pointerFhbnd2 = 0;

  pointerFhbnd1 = lFhbndCoarseTemp1;
  
  int subfaceIndexCurrent_0 = subfaceIndex[0];
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexCurrent_1 = subfaceIndex[1];
  int subintervalIndex_1;
{% endif %}
  
  // This loop decodes the indices of subfaceIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation loop).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; ++l) {
    subintervalIndex_0    = subfaceIndexCurrent_0 % 3;  
    subfaceIndexCurrent_0 = (subfaceIndexCurrent_0 - subintervalIndex_0)/3;
{% if nDim==3 %}
    subintervalIndex_1    = subfaceIndexCurrent_1 % 3;
    subfaceIndexCurrent_1 = (subfaceIndexCurrent_1 - subintervalIndex_1)/3;
{% endif %}

    // Zero the values of of the first pair of pointers.
    std::fill_n(pointerFhbnd1, {{nDof*nDof3D*nVarPad }}, 0.0);

    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
    if (l==1) {
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&lFhbndFine[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
      #pragma forceinline
#endif
          {{gemm_face_F}}(&lFhbndFine[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, lFhbndFine, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 lFhbndFine[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else { // l!= 1
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&pointerFhbnd2[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_F}}(&pointerFhbnd2[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}  
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, pointerFhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 pointerFhbnd2[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    pointerFhbnd2 = pointerFhbnd1;
    // Toggle pointer pairs.
    if (pointerFhbnd1 == lFhbndCoarseTemp1) {
      pointerFhbnd1 = lFhbndCoarseTemp2;
    } else {
      pointerFhbnd1 = lFhbndCoarseTemp1;
    }
  }

  std::copy_n(pointerFhbnd2,{{nDof*nDof3D*nVarPad}},lFhbndCoarse);

}



void {{codeNamespace}}::volumeUnknownsProlongation_old(
    double* restrict luhFine,
    const double* restrict luhCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subcellIndex
){
  const int levelDelta = fineGridLevel - coarseGridLevel;

  double luhFineTemp[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  
{% if useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerUh1 = 0;
  double * pointerUh2 = 0;

  // This ensures that the first pointer 
  // points to luhFine in the last iteration
  // of the following loop.
  if (levelDelta % 2 == 0) {
    pointerUh1 = luhFineTemp;
  } else {
    pointerUh1 = luhFine;
  }
  
  int subcellIndexPrevious_0 = subcellIndex[0];
  int subcellIndexCurrent_0;
  int subintervalIndex_0;
  int subcellIndexPrevious_1 = subcellIndex[1];
  int subcellIndexCurrent_1;
  int subintervalIndex_1;
{% if nDim==3 %}
  int subcellIndexPrevious_2 = subcellIndex[2];
  int subcellIndexCurrent_2;
  int subintervalIndex_2;
{% endif %}

  // This loop step by step decodes the elements of subcellIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; ++l) {
    const int significance = powOf3(levelDelta-l);
    subcellIndexCurrent_0 = subcellIndexPrevious_0 % significance;
    subintervalIndex_0    = (subcellIndexPrevious_0 - subcellIndexCurrent_0)/significance;
    subcellIndexCurrent_1 = subcellIndexPrevious_1 % significance;
    subintervalIndex_1    = (subcellIndexPrevious_1 - subcellIndexCurrent_1)/significance;
{% if nDim==3 %}
    subcellIndexCurrent_2 = subcellIndexPrevious_2 % significance;
    subintervalIndex_2    = (subcellIndexPrevious_2 - subcellIndexCurrent_2)/significance;
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerUh1, {{nDof*nDof*nDof3D*nData}}, 0.0);

    // Apply the single level prolongation operator.
    // Use the coarse level unknowns as input in the first iteration.
    if (l==1) {
      //singleLevelVolumeUnknownsProlongation<{{nData}}>(pointerUh1, luhCoarse, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&luhCoarse[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d]
                            += luhCoarse[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d] *
{% if nDim==3 %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3] *
{% endif %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2] *
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else {
      //singleLevelVolumeUnknownsProlongation<{{nData}}>(pointerUh1, pointerUh2, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&pointerUh2[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d]
                            += pointerUh2[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d] *
{% if nDim==3 %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3] *
{% endif %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2] *
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    subcellIndexPrevious_0 = subcellIndexCurrent_0;
    subcellIndexPrevious_1 = subcellIndexCurrent_1;
{% if nDim==3 %}
    subcellIndexPrevious_2 = subcellIndexCurrent_2;
{% endif %}

    pointerUh2 = pointerUh1;

    // Toggle pointers.
    if (pointerUh1 == luhFineTemp) {
      pointerUh1 = luhFine;
    } else {
      pointerUh1 = luhFineTemp;
    }
  }

}


void {{codeNamespace}}::volumeUnknownsRestriction_old(
    double* restrict luhCoarse,
    const double* restrict luhFine,
    const int coarseGridLevel,
    const int fineGridLevel,
    const int* const subcellIndex
) {
  const int levelDelta    = fineGridLevel - coarseGridLevel;

  double luhCoarseTemp1[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  double luhCoarseTemp2[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  
{% if useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerUh1 = 0;
  double * pointerUh2 = 0;

  pointerUh1 = luhCoarseTemp1;

  int subcellIndexCurrent_0 = subcellIndex[0];
  int subintervalIndex_0;
  int subcellIndexCurrent_1 = subcellIndex[1];
  int subintervalIndex_1;
{% if nDim==3 %}
  int subcellIndexCurrent_2 = subcellIndex[2];
  int subintervalIndex_2;
{% endif %}
  // This loop step by step decodes the elements of subcellIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation decoding).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; ++l) {
    subintervalIndex_0    = subcellIndexCurrent_0 % 3;
    subcellIndexCurrent_0 = (subcellIndexCurrent_0 - subintervalIndex_0)/3;
    subintervalIndex_1    = subcellIndexCurrent_1 % 3;
    subcellIndexCurrent_1 = (subcellIndexCurrent_1 - subintervalIndex_1)/3;
{% if nDim==3 %}
    subintervalIndex_2    = subcellIndexCurrent_2 % 3;
    subcellIndexCurrent_2 = (subcellIndexCurrent_2 - subintervalIndex_2)/3;
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerUh1, {{nDof*nDof*nDof3D*nData}}, 0.0);

    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
    if (l==1) {
      //singleLevelVolumeUnknownsRestriction<{{nData}}>(pointerUh1, luhFine, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&luhFine[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d] +=
                        luhFine[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d]
{% if nDim==3 %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else {
      //singleLevelVolumeUnknownsRestriction<{{nData}}>(pointerUh1, pointerUh2, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&pointerUh2[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d] +=
                        pointerUh2[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d]
{% if nDim==3 %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    pointerUh2 = pointerUh1;

    // Toggle the addresses of the pointers.
    if (pointerUh1 == luhCoarseTemp1) {
      pointerUh1 = luhCoarseTemp2;
    } else {
      pointerUh1 = luhCoarseTemp1;
    }
  }

  // Add restricted fine level unknowns to coarse level unknowns.
  #pragma omp simd aligned(luhCoarse,pointerUh2:ALIGNMENT)
  for (int it = 0; it < {{nDof*nDof*nDof3D*nData}}; it++) {
    luhCoarse[it] += pointerUh2[it];
  }

}
*/
