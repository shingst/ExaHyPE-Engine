void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_5_7_4_dg2fv_x(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm13, 0(%%rdx)\n\t"
                       "vmovapd %%ymm14, 64(%%rdx)\n\t"
                       "vmovapd %%ymm15, 128(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $128, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm13, %%xmm13, %%xmm13\n\t"
                       "vxorpd %%xmm14, %%xmm14, %%xmm14\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 32(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 64(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 40(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 72(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 48(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 80(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 56(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 88(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm13, 0(%%rdx)\n\t"
                       "vmovsd %%xmm14, 64(%%rdx)\n\t"
                       "vmovsd %%xmm15, 128(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $152, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $152, %%rdx\n\t"
                       "addq $96, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $6, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm15, 0(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $128, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm15, 0(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $152, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $24, %%rdx\n\t"
                       "addq $32, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $7, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 280;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_5_7_4_dg2fv_y(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd %%ymm13, 0(%%rdx)\n\t"
                       "vmovupd %%ymm14, 280(%%rdx)\n\t"
                       "vmovupd %%ymm15, 560(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $1760, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm13, %%xmm13, %%xmm13\n\t"
                       "vxorpd %%xmm14, %%xmm14, %%xmm14\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 32(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 64(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 40(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 72(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 48(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 80(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 56(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 88(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm13, 0(%%rdx)\n\t"
                       "vmovsd %%xmm14, 280(%%rdx)\n\t"
                       "vmovsd %%xmm15, 560(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $1784, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $800, %%rdx\n\t"
                       "addq $96, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $6, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd %%ymm15, 0(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $1760, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm15, 0(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $1784, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $240, %%rdx\n\t"
                       "addq $32, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $7, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 280;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_5_4_7_fv2dg_x(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 128(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 136(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 144(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 152(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 96(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 160(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 104(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 168(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 112(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 176(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm13, 0(%%rdx)\n\t"
                       "vmovapd %%ymm14, 64(%%rdx)\n\t"
                       "vmovapd %%ymm15, 128(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $248, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm13, %%xmm13, %%xmm13\n\t"
                       "vxorpd %%xmm14, %%xmm14, %%xmm14\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 64(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 128(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 72(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 136(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 80(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 144(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 88(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 152(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 32(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 96(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 160(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 40(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 104(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 168(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 48(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 112(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 176(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm13, 0(%%rdx)\n\t"
                       "vmovsd %%xmm14, 64(%%rdx)\n\t"
                       "vmovsd %%xmm15, 128(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $272, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $152, %%rdx\n\t"
                       "addq $192, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $3, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm15, 0(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $248, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 32(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 40(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 48(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm15, 0(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $272, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $24, %%rdx\n\t"
                       "addq $64, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $4, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 280;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_5_4_7_fv2dg_y(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 128(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 136(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 144(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 152(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 96(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 160(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 104(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 168(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 112(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 176(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd %%ymm13, 0(%%rdx)\n\t"
                       "vmovupd %%ymm14, 160(%%rdx)\n\t"
                       "vmovupd %%ymm15, 320(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $1760, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm13, %%xmm13, %%xmm13\n\t"
                       "vxorpd %%xmm14, %%xmm14, %%xmm14\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 64(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 128(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 72(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 136(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 80(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 144(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 88(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 152(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 32(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 96(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 160(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 40(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 104(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 168(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 48(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 112(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 176(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm13, 0(%%rdx)\n\t"
                       "vmovsd %%xmm14, 160(%%rdx)\n\t"
                       "vmovsd %%xmm15, 320(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $1784, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $440, %%rdx\n\t"
                       "addq $192, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $3, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd %%ymm15, 0(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $1760, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 32(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 40(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmovsd 48(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm15, 0(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $1784, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $120, %%rdx\n\t"
                       "addq $64, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $4, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 280;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_5_4_4_uh2lob_x(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm13, %%ymm13\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm1\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm2\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm13, 0(%%rdx)\n\t"
                       "vmovapd %%ymm14, 64(%%rdx)\n\t"
                       "vmovapd %%ymm15, 128(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $128, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm13, %%xmm13, %%xmm13\n\t"
                       "vxorpd %%xmm14, %%xmm14, %%xmm14\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 32(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 64(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 40(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 72(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 48(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 80(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm3\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm3, %%xmm0, %%xmm4\n\t"
                       "vaddsd %%xmm4, %%xmm13, %%xmm13\n\t"
                       "vmovsd 56(%%rsi), %%xmm1\n\t"
                       "vmulsd %%xmm3, %%xmm1, %%xmm5\n\t"
                       "vaddsd %%xmm5, %%xmm14, %%xmm14\n\t"
                       "vmovsd 88(%%rsi), %%xmm2\n\t"
                       "vmulsd %%xmm3, %%xmm2, %%xmm6\n\t"
                       "vaddsd %%xmm6, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm13, 0(%%rdx)\n\t"
                       "vmovsd %%xmm14, 64(%%rdx)\n\t"
                       "vmovsd %%xmm15, 128(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $152, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $152, %%rdx\n\t"
                       "addq $96, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $3, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $4, %%r12\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovupd 0(%%rdi), %%ymm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm2\n\t"
                       "vaddpd %%ymm2, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm15, 0(%%rdx)\n\t"
                       "addq $32, %%rdx\n\t"
                       "subq $128, %%rdi\n\t"
                       "cmpq $4, %%r12\n\t"
                       "jl 34b\n\t"
                       "34:\n\t"
                       "addq $1, %%r12\n\t"
                       "vxorpd %%xmm15, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 0(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 8(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 16(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd 0(%%rdi), %%xmm1\n\t"
                       "addq $40, %%rdi\n\t"
                       "vmovsd 24(%%rsi), %%xmm0\n\t"
                       "vmulsd %%xmm1, %%xmm0, %%xmm2\n\t"
                       "vaddsd %%xmm2, %%xmm15, %%xmm15\n\t"
                       "vmovsd %%xmm15, 0(%%rdx)\n\t"
                       "addq $8, %%rdx\n\t"
                       "subq $152, %%rdi\n\t"
                       "cmpq $5, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $24, %%rdx\n\t"
                       "addq $32, %%rsi\n\t"
                       "subq $40, %%rdi\n\t"
                       "cmpq $4, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 160;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_8_4_4_uh2lob_y_slice(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $8, %%r12\n\t"
                       "vxorpd %%ymm10, %%ymm10, %%ymm10\n\t"
                       "vxorpd %%ymm11, %%ymm11, %%ymm11\n\t"
                       "vxorpd %%ymm12, %%ymm12, %%ymm12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm10, 0(%%rdx)\n\t"
                       "vmovapd %%ymm11, 32(%%rdx)\n\t"
                       "vmovapd %%ymm12, 64(%%rdx)\n\t"
                       "vmovapd %%ymm13, 96(%%rdx)\n\t"
                       "vmovapd %%ymm14, 128(%%rdx)\n\t"
                       "vmovapd %%ymm15, 160(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $960, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $128, %%rdx\n\t"
                       "addq $96, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $3, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $8, %%r12\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $256, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm14, 0(%%rdx)\n\t"
                       "vmovapd %%ymm15, 32(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $960, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $0, %%rdx\n\t"
                       "addq $32, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $4, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 256;
#endif
}

void Euler::EulerSolver_ADERDG_kernels::aderdg::gemm_8_7_4_dg2fv_y_slice(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "33:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $8, %%r12\n\t"
                       "vxorpd %%ymm10, %%ymm10, %%ymm10\n\t"
                       "vxorpd %%ymm11, %%ymm11, %%ymm11\n\t"
                       "vxorpd %%ymm12, %%ymm12, %%ymm12\n\t"
                       "vxorpd %%ymm13, %%ymm13, %%ymm13\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 64(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 72(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 80(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 88(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm10, 0(%%rdx)\n\t"
                       "vmovapd %%ymm11, 32(%%rdx)\n\t"
                       "vmovapd %%ymm12, 64(%%rdx)\n\t"
                       "vmovapd %%ymm13, 96(%%rdx)\n\t"
                       "vmovapd %%ymm14, 128(%%rdx)\n\t"
                       "vmovapd %%ymm15, 160(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $1728, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $128, %%rdx\n\t"
                       "addq $96, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $6, %%r13\n\t"
                       "jl 33b\n\t"
                       "33:\n\t"
                       "addq $1, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "34:\n\t"
                       "addq $8, %%r12\n\t"
                       "vxorpd %%ymm14, %%ymm14, %%ymm14\n\t"
                       "vxorpd %%ymm15, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vmovapd 0(%%rdi), %%ymm1\n\t"
                       "vmovapd 32(%%rdi), %%ymm2\n\t"
                       "vmulpd %%ymm1, %%ymm0, %%ymm3\n\t"
                       "vaddpd %%ymm3, %%ymm14, %%ymm14\n\t"
                       "addq $448, %%rdi\n\t"
                       "vmulpd %%ymm2, %%ymm0, %%ymm4\n\t"
                       "vaddpd %%ymm4, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm14, 0(%%rdx)\n\t"
                       "vmovapd %%ymm15, 32(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $1728, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 34b\n\t"
                       "addq $0, %%rdx\n\t"
                       "addq $32, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $7, %%r13\n\t"
                       "jl 33b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("LIBXSMM KERNEL COMPILATION ERROR in: " __FILE__)
#error No kernel was compiled, lacking support for current architecture?
#endif

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 448;
#endif
}

